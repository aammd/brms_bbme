[
  {
    "objectID": "topics/intercept_only/index.html",
    "href": "topics/intercept_only/index.html",
    "title": "Fitting an intercept-only model",
    "section": "",
    "text": "Its very useful to have an idea of where the variation is in a dataset, as a guide to model building and future data collection. Before collecting information on independent variables that you think might explain variation – first find out how much variation there is!\nRandom intercepts give us a very handy way to investigate this: the so-called “intercept only” model. At this point in the course we now have all the tools needed to build it. There are two steps: first we build a model with no predictors at all, only a random intercept for every grouping variable in the data (e.g. species, sites, years, regions). Then we examine the relative magnitudes of the standard deviations to see which is relatively larger or smaller."
  },
  {
    "objectID": "topics/intercept_only/index.html#variance-partitioning-with-hierarchical-models",
    "href": "topics/intercept_only/index.html#variance-partitioning-with-hierarchical-models",
    "title": "Fitting an intercept-only model",
    "section": "",
    "text": "Its very useful to have an idea of where the variation is in a dataset, as a guide to model building and future data collection. Before collecting information on independent variables that you think might explain variation – first find out how much variation there is!\nRandom intercepts give us a very handy way to investigate this: the so-called “intercept only” model. At this point in the course we now have all the tools needed to build it. There are two steps: first we build a model with no predictors at all, only a random intercept for every grouping variable in the data (e.g. species, sites, years, regions). Then we examine the relative magnitudes of the standard deviations to see which is relatively larger or smaller."
  },
  {
    "objectID": "topics/intercept_only/index.html#abundance-of-mites-in-different-samples",
    "href": "topics/intercept_only/index.html#abundance-of-mites-in-different-samples",
    "title": "Fitting an intercept-only model",
    "section": "Abundance of mites in different samples",
    "text": "Abundance of mites in different samples\nWe’re going to build this by extending the observation level random effect model from the previous section. Remember that observation-level random effects are mostly useful for Poisson distributions! If you want to extend this model to other kinds of data, remember to remove that part of it.\n\nMathematical model\n\\[\n\\begin{align}\n\\text{Abundance}_i &\\sim \\text{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} &\\sim \\mu + \\beta_{\\text{sample}[i]} + \\beta_{\\text{species[i]}} + \\beta_i\\\\\n\\mu &\\sim \\text{Normal}(3, 1)\\\\\n\\beta_{\\text{sample}} &\\sim \\text{Normal}(0,  \\sigma_{\\text{samp}})\\\\\n\\beta_{\\text{species}} &\\sim \\text{Normal}(0, \\sigma_{\\text{species}})\\\\\n\\beta_i &\\sim \\text{Normal}(0,                \\sigma_{\\text{obs}}) \\\\\n\\sigma_{\\text{samp}}    &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{species}} &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{obs}}     &\\sim \\text{Exponential}(3)\n\\end{align}\n\\]\n\n\nLoad packages and prepare data\n\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tidybayes)\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.23.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:tidybayes':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\ndata(\"mite\", package = \"vegan\")\n\n\nspp_names &lt;- colnames(mite)\nspp_names &lt;- setNames(1:ncol(mite), colnames(mite))\n\n\nmite_long &lt;- mite |&gt; \n  mutate(site_id = seq_len(nrow(mite))) |&gt; \n  tidyr::pivot_longer(-site_id,\n                      names_to = \"spp\",\n                      values_to = \"abd\") |&gt; \n  dplyr::mutate(spp_id = spp_names[spp])\n\nknitr::kable(head(mite_long))\n\n\n\n\nsite_id\nspp\nabd\nspp_id\n\n\n\n\n1\nBrachy\n17\n1\n\n\n1\nPHTH\n5\n2\n\n\n1\nHPAV\n5\n3\n\n\n1\nRARD\n3\n4\n\n\n1\nSSTR\n2\n5\n\n\n1\nProtopl\n1\n6"
  },
  {
    "objectID": "topics/intercept_only/index.html#exercise",
    "href": "topics/intercept_only/index.html#exercise",
    "title": "Fitting an intercept-only model",
    "section": "EXERCISE",
    "text": "EXERCISE\ncreate a Poisson model of species abundance that offers three random intercepts: one for species, one for site, and another for observation level. TIP: the dataset has no factor for observation-level variance, you will have to create one.\n\n## data\nmite_long_siteid &lt;- mite_long |&gt; \n  mutate(obs_id = seq_along(spp))\n\n# formula\nmite_intercept_only_bf &lt;- bf(abd ~ 1 + (1 | spp_id) + (1 | site_id) + (1 | obs_id), family = poisson(link = \"log\"))\n\nget_prior(mite_intercept_only_bf, data = mite_long_siteid)\n\n                   prior     class      coef   group resp dpar nlpar lb ub tag\n student_t(3, -2.3, 2.5) Intercept                                            \n    student_t(3, 0, 2.5)        sd                                    0       \n    student_t(3, 0, 2.5)        sd            obs_id                  0       \n    student_t(3, 0, 2.5)        sd Intercept  obs_id                  0       \n    student_t(3, 0, 2.5)        sd           site_id                  0       \n    student_t(3, 0, 2.5)        sd Intercept site_id                  0       \n    student_t(3, 0, 2.5)        sd            spp_id                  0       \n    student_t(3, 0, 2.5)        sd Intercept  spp_id                  0       \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\nmite_intercept_only_prior &lt;- c(\n  prior(normal(3, 1), class = \"Intercept\"),\n  prior(exponential(3), class = \"sd\")\n)\n\nmite_intercept_only_brm &lt;- brm(\n  mite_intercept_only_bf,\n  data = mite_long_siteid,\n  prior = mite_intercept_only_prior,\n  sample_prior = \"yes\",\n  file = here::here(\"topics/intercept_only/mite_intercept_only_brm.rds\"),\n  cores = 4)\n\nTo examine the relative magnitudes of the standard deviations\n\nhead(tidybayes::get_variables(mite_intercept_only_brm))\n\n[1] \"b_Intercept\"           \"sd_obs_id__Intercept\"  \"sd_site_id__Intercept\"\n[4] \"sd_spp_id__Intercept\"  \"Intercept\"             \"r_obs_id[1,Intercept]\"\n\nbayesplot::mcmc_areas(mite_intercept_only_brm, \n                      regex_pars = \"sd_.*\")\n\n\n\n\n\n\n\n\nWhat’s the average abundance of each species? To answer this question, we can extract only the random effects of species, and create a kind of rank-abundance plot. Note that we don’t need to transform the response, because tidybayes::add_epred_rvars applies the link function for us:\n\nmite_long_siteid |&gt; \n  select(spp, spp_id) |&gt; \n  distinct() |&gt; \n  tidybayes::add_epred_rvars(\n    mite_intercept_only_brm, \n    re_formula = ~ (1 | spp_id)\n    ) |&gt; \n  mutate(avg_abd=.epred,\n         spp = forcats::fct_reorder(.f = spp, .x = .epred)) |&gt;\n  ggplot(aes(y = spp, xdist = .epred)) + \n  stat_dist_pointinterval()\n\n\n\n\n\n\n\n\nPosterior predictions look alright also:\n\npp_check(mite_intercept_only_brm) + coord_cartesian(xlim = c(0,100))\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nCoordinate system already present.\nℹ Adding new coordinate system, which will replace the existing one."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course contents",
    "section": "",
    "text": "This course is based on Modèles hiérarchiques pour les sciences de la vie by Guillaume Blanchet and Andrew MacDonald."
  },
  {
    "objectID": "index.html#day-1-introduction-data-simulation-discrete-models",
    "href": "index.html#day-1-introduction-data-simulation-discrete-models",
    "title": "Course contents",
    "section": "Day 1: Introduction, data simulation, discrete models",
    "text": "Day 1: Introduction, data simulation, discrete models\n\n\n\nSlides\n\nBayesian and MCMC\nStan and HMC\nProbability Distributions\n\n\n\n\nExercises\n\nSimulation and model fitting in Stan"
  },
  {
    "objectID": "index.html#day-2",
    "href": "index.html#day-2",
    "title": "Course contents",
    "section": "Day 2",
    "text": "Day 2\n\n\n\nSlides\n\nData we’ll be using\nLinear models\n\n\n\n\nExercises\n\nDiscrete predictors\nLinear regression"
  },
  {
    "objectID": "index.html#day-3",
    "href": "index.html#day-3",
    "title": "Course contents",
    "section": "Day 3",
    "text": "Day 3\n\n\n\nSlides\n\nSimple hierarchical models\n\n\n\n\nExercises\n\nHierarchy on the intercept"
  },
  {
    "objectID": "index.html#day-4",
    "href": "index.html#day-4",
    "title": "Course contents",
    "section": "Day 4",
    "text": "Day 4\n\n\n\nSlides\n\nComplex hierarchical models\n\n\n\n\nExercises\n\nHierarchy on the intercept\noffsets\nIntercept-Only Model\nHierarchy on the slope"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html",
    "href": "slides/nn_extra_slides/index.html",
    "title": "Extra stuff",
    "section": "",
    "text": "The square matrix has as many rows at it has columns \\[\n\\mathbf{B} = \\begin{bmatrix}\n                B_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n                B_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#square-matrix",
    "href": "slides/nn_extra_slides/index.html#square-matrix",
    "title": "Extra stuff",
    "section": "",
    "text": "The square matrix has as many rows at it has columns \\[\n\\mathbf{B} = \\begin{bmatrix}\n                B_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n                B_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "href": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "title": "Extra stuff",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nNot sure if it should be included or not"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "href": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "title": "Extra stuff",
    "section": "Eigenvectors and eigenvalues",
    "text": "Eigenvectors and eigenvalues\n\nRight eigenvector is :\n\\[\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}\\] Left eigenvector is :\n\\[\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}\\]\nRules\n\n\\(\\mathbf{A}\\) has to be a square matrix\nIf \\(\\mathbf{w}\\) is an eigenvector of \\(\\mathbf{A}\\), so is \\(c\\mathbf{w}\\) for any value of \\(c \\neq0\\)\nThe right eigenvector of \\(\\mathbf{A}^T\\) is the left eigenvector of \\(\\mathbf{A}\\)\nEigenvectors are linearly independent"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive definite matrix",
    "text": "Positive definite matrix\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\nError: Matrix X is not positive definite\n\nor similarly\n\nError: Matrix X is not positive semi-definite\n\nWhat does this mean ? Any idea ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\n\nNerdy mathematical definition\nPositive definite matrix\n\\(\\mathbf{M}\\) is a positive definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} &gt; 0\\)\nPositive semi-definite matrix\n\\(\\mathbf{M}\\) is a positive semi-definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\n\nChecking if a matrix is positive (semi-)definite\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\nAll we have to do is look at the eigenvalue of a square matrix.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than 0, matrix \\(\\mathbf{M}\\) is positive definite.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than or equal ro 0, matrix \\(\\mathbf{M}\\) is positive semi-definite."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#dot-product",
    "href": "slides/nn_extra_slides/index.html#dot-product",
    "title": "Extra stuff",
    "section": "Dot product",
    "text": "Dot product\n\\[\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n\\]\n\n\\[\n            \\begin{bmatrix}\n                3 & 1\\\\\n            \\end{bmatrix}\n            \\cdot\n            \\begin{bmatrix}\n                2\\\\ 5\\\\\n            \\end{bmatrix} =\n                3 \\times  2 + 1 \\times 5 = 11\n\\]\n\nIn R\n\nv &lt;- c(3, 1)\nx &lt;- c(2,5)\nsum(v * x)\n\n[1] 11"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\\[\n\\begin{align*}\n        1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n        0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n        1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n    \\end{align*}\n\\] \\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we mathematically solve for \\(\\boldsymbol{\\beta}\\)?\n\\[\n    \\begin{align*}\n        \\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n    \\end{align*}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we solve for \\(\\boldsymbol{\\beta}\\) in R?\n\nX &lt;- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny &lt;- c(1, 0, 1)\n\n(beta &lt;- solve(X, y))\n\n[1]  0.20000000  0.05714286 -0.02857143"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n\nConjugate priors\nThese types of priors are convenient to use because\n\nThey are computationally faster to use\nThey can be interepreted as additional data\n\n\nWhy are they useful?\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n\nConjugate priors\n\nWhat does it mean to be of the same functional form?\nIt means that both distribution have th same mathematical structure.\n\n\nBinomial distribution \\[\\theta^a(1-\\theta)^b\\]\n\nBeta distribution \\[\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\n\n\nhttps://en.wikipedia.org/wiki/Conjugate_prior"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "href": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "title": "Extra stuff",
    "section": "Move this in another place",
    "text": "Move this in another place\nTechnically, we can sample all \\(\\boldsymbol{\\beta}_{f[l]}\\) independently, however, using multivariate Gaussian distribution, we can sample the \\(\\boldsymbol{\\beta}_{f}\\) for all levels of the factor in one go as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\] where\n\n\\(\\boldsymbol{\\mu}_{f}\\) is a vector of \\(k\\) means, one for each level of the factor\n\\(\\mathbf{D}_f\\) is a \\(k\\times k\\) diagonal matrix with variance term on the diagonal"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nThe structure of matrix \\(\\mathbf{D}_f\\) can be considered in two different ways in\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\]\nWritten in the general form as we did in the equation above, we assume that all variance on the diagonal are potentially different. Or in other words, the variance in each group is assumed to be different\n\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f[1]} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f[2]} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f[l]} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f[k]}\\\\\n        \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nHowever, it can be assumed to be all the same variance regardless of the group considered\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n        \\end{bmatrix}\\]\n\nIn this case, \\(\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\) can be rewritten as \\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})\\] Note: This is essentially the same thing as a one-way analysis of variance."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nAs discuss yesterday, a linear model can be writen as\n\\[(\\mathbf{y}|\\mathbf{X}, \\boldsymbol{\\beta}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma\\mathbf{y}^2\\mathbf{I})\\]\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory varaibles)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation-1",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation-1",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is another matrix of explanatory variables with \\(n\\) rows (samples) and \\(q\\) columns (explanatory variables)\n\\(\\mathbf{b}\\) is a vector \\(q\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{Z}\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation-2",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation-2",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})\\] What is also noticeable in this model is the conditional relationship between \\(\\mathbf{y}\\) and \\(\\mathbf{b}\\).\nSpecifically, in this formulation,\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] where \\(\\mathbf{\\Sigma}\\) is a covariance matrix.\nBased on this general formulation, we can now define all unconstrained hierarchical models."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nFor simplicity, let’s assume that we are monitoring the behaviour of the mallard every minutes and that we are recording whether it is\n\nOn land\n\n\n\n\n\n\n\nIn the water"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-1",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nUsing this information, we can draw diagram defining how the behaviour of the mallard changes at every time steps\n. . ."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-2",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-2",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nIn a markov chain, we assume that we know how probable it is to go from one behaviour (land) to another (water)\n\n. . ."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-3",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-3",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nIn an MCMC, we assume that the likeliness of passing from one behaviour (land) to another (water) depends on a statistical distribution.\n\n. . ."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#addition-and-substraction",
    "href": "slides/nn_extra_slides/index.html#addition-and-substraction",
    "title": "Extra stuff",
    "section": "Addition and Substraction",
    "text": "Addition and Substraction\n\\[\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}\\] \\[C_{ij} = A_{ij} \\pm B_{ij}\\]\n\n\\[\\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} +\n            \\begin{bmatrix}\n                2 & 1\\\\\n                4 & -2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                3+2 & 5+1\\\\\n                1+4 & -2-2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                5 & 6\\\\\n                5 & -4\\\\\n            \\end{bmatrix}\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB &lt;- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#triangular-matrix",
    "href": "slides/nn_extra_slides/index.html#triangular-matrix",
    "title": "Extra stuff",
    "section": "Triangular matrix",
    "text": "Triangular matrix\n\n\n\nLower triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}\\]\n\n\n\nUpper triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#symmetric-matrix",
    "href": "slides/nn_extra_slides/index.html#symmetric-matrix",
    "title": "Extra stuff",
    "section": "Symmetric matrix",
    "text": "Symmetric matrix\nThe values on the above and below the diagonal are match so that \\(A = A^t\\)\n\n\\[\n\\begin{bmatrix}\n                3 & 4 & -10\\\\\n                4 & 5 & 7\\\\\n                -10 & 7 & -6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#matrix-inversion",
    "href": "slides/nn_extra_slides/index.html#matrix-inversion",
    "title": "Extra stuff",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix \\(\\mathbf{A}\\) is defined as \\(\\mathbf{A}^{-1}\\)\nAs such, \\[\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\]\n\n. . .\n\nIn R\n\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv &lt;- solve(A))\n\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n\nA %*% Ainv\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#matrix-inversion-1",
    "href": "slides/nn_extra_slides/index.html#matrix-inversion-1",
    "title": "Extra stuff",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nInverting a diagonal matrix\n\\[D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n. . .\nMany techniques have been proposed to estimate the parameters of a regression model.\n. . .\nThe goal of this course is not to study these techniques but we will learn how to play with the estimated parameters because it will be very useful as we move along.\n. . .\nThe most common way to build a regression model is\n\nreg &lt;- lm(b.exemplaris ~ humidity)\n\n. . .\nSay we now want to build a model’s confidence interval from a linear regression\n. . .\nHow would you do it ?\n. . .\nLet’s look at the model’s results, maybe it will help us"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-1",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-1",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nModel’s results\n\n\n(summaryReg &lt;- summary(reg))\n\n\nCall:\nlm(formula = b.exemplaris ~ humidity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47988 -0.26475  0.00611  0.32590  1.36077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.57389    0.04720   54.53   &lt;2e-16 ***\nhumidity     1.10086    0.07976   13.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4718 on 98 degrees of freedom\nMultiple R-squared:  0.6603,    Adjusted R-squared:  0.6569 \nF-statistic: 190.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n. . .\n\nLet’s say we want to construst the model’s confidence intervals by sampling multiple times (say 100 times!) the regression parameters, which we will assume follow Gaussian distribution. How would you do this?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-2",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-2",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nWe could sample the model parameters but how can we do this properly?\n. . .\nAny suggestions?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-3",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-3",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nIf we look at the estimated regression model coefficient, we can learn a few things\n. . .\n\nsummaryReg$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 2.573889 0.04720304 54.52804 4.011925e-75\nhumidity    1.100865 0.07975800 13.80256 1.035796e-24\n\n\n. . .\nNotably, there are uncertainty around the parameters.\n. . .\nMaybe we can use this information to sample model parameters and reconstruct models across different iterations of parameters.\n. . .\nLet’s give it a shot !"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-4",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-4",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nIf we assume that the parameters of our particular model follow a Gaussian distribution, we can state that\n. . .\n\\[\\beta_0 \\sim \\mathcal{N}(2.574, 0.047^2)\\] \\[\\beta_1 \\sim \\mathcal{N}(1.101, 0.080^2)\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-5",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-5",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nIn R, we can do this as follow\n\n# Object that include regression coefficients\nregCoef &lt;- summaryReg$coefficients\n\n# Sample regression parameters\nbeta_0 &lt;- rnorm(100, mean = regCoef[1,1], sd = regCoef[1,2])\nbeta_1 &lt;- rnorm(100, mean = regCoef[2,1], sd = regCoef[2,2])"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-6",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-6",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-7",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-7",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nBut is this the right way to do it ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-8",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-8",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nActually, even if the model’s confidence interval look about right, they are wrong !"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-9",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-9",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nThe approach presented in the previous slide works only if we assume that the parameters are completely independent from one another.\n. . .\nA situation that happens only in very specific circumstances.\n. . .\nSo… We need to find a way to account for the non-independencies between the parameters.\n. . .\nHow can we do this ? Any ideas ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-10",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-10",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\n\nAssuming the regression parameters are normally distributed is not a bad assumption.\n\n. . .\n\nHowever to consider a dependencies between the parameters we need to sample them from a multivariate normal distribution where the variance of each parameter and their dependency is defined by a covariance matrix estimated specifically for the data we model.\n\n. . .\n\nThe good news is that this covariance matrix is given by summary.lm function\n\n. . .\n\n\n(covReg &lt;- summaryReg$cov.unscaled)\n\n              (Intercept)      humidity\n(Intercept)  0.0100098513 -0.0005305969\nhumidity    -0.0005305969  0.0285782940"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-11",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-11",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nFor our specific model, mathematically, we assume that\n. . .\n\\[\\begin{bmatrix}\n  \\beta_0\\\\\n  \\beta_1\\\\\n\\end{bmatrix} \\sim \\mathcal{MVN} \\left( \\begin{bmatrix}\n  2.574\\\\\n  1.101\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0.0100 & -0.0005 \\\\\n  -0.0005 & 0.0286 \\\\\n\\end{bmatrix} \\right)\\]\n. . .\nNote To present the multivariate normal distribution, we rely on matrix notation. This is our first introduction into matrix algebra. We will talk about this more into this course."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-12",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-12",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters\nIn R, we can sample the parameters using a multivariate normal distribution using the following code\n\n# Object that include regression coefficients\nregCoef &lt;- summaryReg$coefficients\n\n# Sample regression parameters\nbeta &lt;- MASS::mvrnorm(100, regCoef[,1], Sigma = covReg)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-13",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-13",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-14",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-14",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nSampling model parameters - Comparison"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html",
    "href": "slides/02_5_Distribution/index.html",
    "title": "Probability Distribution",
    "section": "",
    "text": "To understand distributions, we first need to have a basic understanding of probabilities."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities",
    "href": "slides/02_5_Distribution/index.html#probabilities",
    "title": "Probability Distribution",
    "section": "",
    "text": "To understand distributions, we first need to have a basic understanding of probabilities."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#a-bit-of-history",
    "href": "slides/02_5_Distribution/index.html#a-bit-of-history",
    "title": "Probability Distribution",
    "section": "A bit of history",
    "text": "A bit of history\n\nUnlike many other fields of science, the first contributors in the study of probability were not scholars, they were gamblers !\n\n. . .\n\nFor example, the emperor Claudius (10 BC – 54 AD), who was an avid gambler (he had a carriage built to allow him and his party to gamble while travelling) wrote a treatise on randomness and probability.\n\n\n\n\n\n\n\n\n\n\n\nLanciani (1892) Gambling and Cheating in Ancient Rome. The North American Review 155:97-105"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#a-bit-of-history-1",
    "href": "slides/02_5_Distribution/index.html#a-bit-of-history-1",
    "title": "Probability Distribution",
    "section": "A bit of history",
    "text": "A bit of history\n\nIf you want a fun book to read about probabilities, its history and the difficulty of working with probabilities, I strongly recommend"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#the-basics-of-probabilities",
    "href": "slides/02_5_Distribution/index.html#the-basics-of-probabilities",
    "title": "Probability Distribution",
    "section": "The basics of probabilities",
    "text": "The basics of probabilities\nA probability ALWAYS ranges between 0 and 1\n. . .\nA probability of 0 means that an event is impossible\n. . .\n\nExample: The probability that a dog and a cat naturally reproduce is 0\n\n. . .\nA probability of 1 means that an event is certain\n. . .\n\nExample: The probability that you are in this summer school as we speak is 1"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#the-basics-of-probabilities-1",
    "href": "slides/02_5_Distribution/index.html#the-basics-of-probabilities-1",
    "title": "Probability Distribution",
    "section": "The basics of probabilities",
    "text": "The basics of probabilities\nNotation\n\nA classic way to write the probability of an event is to use the notation \\(P\\).\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nThe probability that it rains as you walk outside after the lecture is written as\n\\[P(r)\\] where \\(r\\) is the event you are interested in. Here, \\(r\\) is a rain as you walk outside after the lecture today"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\n\nWhen dealing with discrete (countable) events, it is very practical to know the number of events that can occur.\n\n. . .\n\nIn the simplest case, we can either measure the probability of an event to occur or not.\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIt can either rain or not. Mathematically, the probability that it rains is written as\n\\[P(r)\\] and the probability that it does not rain would be written as\n\\[1 - P(r)\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events-1",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events-1",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\n\nUsually, these basic notions of probabilities are presented using coin flipping. When a coin is flip, it is usually assumed that\n\\[P(r)=0.5\\]\n\n. . .\n\nHowever, in probability theory, P(r) can have any value ranging between 0 and 1.\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think is the probability that it will rain at the end of the lecture?"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events-2",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events-2",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\nAt this point we can note that when we add the probabilites of all possible events, the sum will always be 1\n. . .\nExample\n\n\n\n\n\n\n\n\n\n\\[P(r) + (1-P(r)) = 1\\]\n. . .\nThis is true only if the events are independent from each other"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#independent",
    "href": "slides/02_5_Distribution/index.html#independent",
    "title": "Probability Distribution",
    "section": "Independent !?",
    "text": "Independent !?\n\nEvents that are independent from each other means that if an event occurs it is in no way related to the occurrence of another event.\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf we assume that weather events like a rainy day are independent from one another, it means that if it rains today it is unrelated to the weather of yesterday or tomorrow.\n\n. . .\n\nNote : This can be a good or a bad or dangerous assumption to make depending on the problem you are working on."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\n\n\n\n\n\n\n\n\n\nJacob Bernoulli (1655 - 1705)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-1",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-1",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nThe probability distribution (or probability mass function) of the Bernoulli distribution defines the probability of an event to occur given that there is only one other event that can occur (e.g. rain or no rain)\n. . .\nClassically, we will give a value of 1 to one event (no rain) and 0 to the other (rain).\n. . .\nFrom a mathematical perspective, it does not matter which event is given a 1 (or a 0). However, often it is common practice to choose how we give values based on the interpretation we make of the results."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-2",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-2",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nMathematically, the probability mass function of the Bernoulli distribution can be written as\n\\[\\begin{align*}\np \\quad & \\text{if} \\quad x =1\\\\\n(1-p) \\quad & \\text{if}\\quad  x =0\n\\end{align*}\\]\nwhere \\(p\\) is a shorthand for \\(P(x)\\) and \\(x\\) is one of two events."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-interlude",
    "href": "slides/02_5_Distribution/index.html#moment-interlude",
    "title": "Probability Distribution",
    "section": "Moment interlude",
    "text": "Moment interlude\nUsing probability distributions is practical because from them we can derive general information characterizing the each distribution.\n. . .\nThese characteristics are know as moments of a distribution… And you know them :\n. . .\n\nMean\nVariance\nSkewness\nKurtosis\n…"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\n\nFor the sake of conciseness, in this course, we will discuss only the first two moments of distributions.\nMean\n\\[p\\]\nExample\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.14\\) in any given day, it means that, on average in a week (7 days) we should expect it will rain 1 day."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-1",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\n\nVariance\n\\[p(1-p)\\] Example\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.5\\) in any given day, it means that, across multiple weeks, some weeks might have no rain while some weeks it might rain all days because the variance is\n\\[p(1-p)=0.5\\times(1-0.5)=0.25\\quad\\text{and}\\quad \\sqrt{0.25} = 0.5\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-2",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-2",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\nIf you want to go deeper down and learn about the other moments of the Bernoulli distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the Bernoulli distribution\nhttps://en.wikipedia.org/wiki/Bernoulli_distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#lets-make-it-more-complicated",
    "href": "slides/02_5_Distribution/index.html#lets-make-it-more-complicated",
    "title": "Probability Distribution",
    "section": "Let’s make it more complicated",
    "text": "Let’s make it more complicated\nSo far, we focused on a situation where the two events to consider either occur or not.\n. . .\nThere are many problems where interest lies in studying the likeliness of an event occurring over a known number of independent trials.\n. . .\nExample\nHow many rainy day will there be during the five days of our summer school ?"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nPut differently, the binomial distribution is designed to approach questions where we are interested in finding the number of success (e.g. it rains !) out of a known set of independent trials (e.g. the five days of the summer school).\n. . .\nThe binomial distribution is a generalisation of the Bernoulli distribution\n. . .\nIt is a common distribution used when sampling is done with replacement\n. . .\nLet’s take a look at the math of the Binomial distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-1",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-1",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nProbability mass function\n\\[\\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere\n\n\\(n\\) : Number of trails\n\\(k\\) : Number of success (an event occurs)\n\\(p\\) : Probability that an event occurs\n\nNote that \\(n \\ge k\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#mathematical-technicalities-interlude",
    "href": "slides/02_5_Distribution/index.html#mathematical-technicalities-interlude",
    "title": "Probability Distribution",
    "section": "Mathematical technicalities interlude",
    "text": "Mathematical technicalities interlude\n\n\\[\\binom{n}{k}\\]\n\n. . .\n\n\\[\\frac{n!}{k!(n-k)!}\\]\n\n. . .\n\n\\[\\frac{n\\times(n-1)\\times(n-2)\\times\\dots\\times 2\\times 1}{(k\\times(k-1)\\times(k-2)\\times\\dots\\times 2\\times 1)(n-k)\\times(n-k-1)\\times(n-k-2)\\times\\dots\\times 2\\times 1}\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution",
    "title": "Probability Distribution",
    "section": "Moment of the binomial distribution",
    "text": "Moment of the binomial distribution\n\nAgain, for conciseness, we will focus on the first moment (mean) and second moment (variance) of the binomial distribution.\n\n. . .\n\nMean \\[np\\]\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.14\\) in any given day of the 5 days of the summer school, it means that on average we expect it will rain 0.7 days of the summer school (so 1 or no days)\n\\[np = 5 \\times 0.14 = 0.7\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution-1",
    "title": "Probability Distribution",
    "section": "Moment of the binomial distribution",
    "text": "Moment of the binomial distribution\n\nVariance\n\\[np(1-p)\\]\n\n. . .\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.5\\) in any given day, it means that, across multiple weeks (7 days), roughly speaking some weeks might have 1 days of rain while others might have 5 because the variance is\n\\[np(1-p)=7 \\times 0.5\\times(1-0.5)=1.75\\quad\\text{and}\\quad \\sqrt{1.75} = 1.3229\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-binomial-distribution",
    "title": "Probability Distribution",
    "section": "Moments of the binomial distribution",
    "text": "Moments of the binomial distribution\nIf you want to learn more about the other moments of the binomial distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the binomial distribution\nhttps://en.wikipedia.org/wiki/binomial_distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-2",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-2",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe binomial distribution is related to many other probability distribution\n. . .\n\nBernoulli distribution (as we have seen)\n\n. . .\n\nPoisson distribution (when there are an infinite number of trials while \\(np\\) converge to a finite value)\n\n. . .\n\nNormal distribution…\n\n. . ."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#jia-xian-triangle",
    "href": "slides/02_5_Distribution/index.html#jia-xian-triangle",
    "title": "Probability Distribution",
    "section": "Jia Xian triangle",
    "text": "Jia Xian triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-1",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-1",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-2",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-2",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\n\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\n\n. . .\n\nIf \\(n = 3\\)\nWhen \\(k=0\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{0}\\times0.5^0 \\times (1-0.5)^{(3-0)}=0.125\\]\n\n. . .\n\nWhen \\(k=1\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{1}\\times0.5^1 \\times (1-0.5)^{(3-1)}=0.375\\]\n\n. . .\n\nWhen \\(k=2\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{2}\\times0.5^1 \\times (1-0.5)^{(3-2)}=0.375\\]\n\n. . .\n\nWhen \\(k=3\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{3}\\times0.5^1 \\times (1-0.5)^{(3-3)}=0.125\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-3",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-3",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 3\\)\n. . ."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-4",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-4",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 10\\)\n. . ."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-5",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-5",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 50\\)\n. . ."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-6",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-6",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 200\\)\n. . ."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-and-gaussian-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-and-gaussian-distribution",
    "title": "Probability Distribution",
    "section": "Binomial and Gaussian distribution",
    "text": "Binomial and Gaussian distribution\nIf the number trials (\\(n\\)) is large enough, it approximate to a Gaussian distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\n\n\n\n\n\n\n\n\n\n\nCarl Friedrich Gauss (1777-1855)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-1",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-1",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\nUnlike the binomial distribution, the Gaussian distribution is a continuous distribution\n. . .\nIt is the a very common distribution that is underlying many random natural phenomenon and it is the basis of statistical theory\n. . .\nLet’s take a look at the mathematical formulation of the Gaussian distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-2",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-2",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\nProbability density function\n\\[\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\]\nwhere\n\n\\(x\\) : continuous variable of interest\n\\(\\mu\\) : The mean of the distribution\n\\(\\sigma\\) : The standard deviation of the distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution",
    "title": "Probability Distribution",
    "section": "Moment of the Gaussian distribution",
    "text": "Moment of the Gaussian distribution\nMean\n\\[\\mu\\]\nExample\n\n\n\n\n\n\nLet’s say we measure the length of the right wing of individual of this species of (angry) bird, it is expected that the wing length will follow a Gaussian distribution with a mean of \\(\\mu\\). We will look at this in more details in the practical exercices later today."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution-1",
    "title": "Probability Distribution",
    "section": "Moment of the Gaussian distribution",
    "text": "Moment of the Gaussian distribution\nVariance\n\\[\\sigma^2\\]\nExample\n\n\n\n\n\n\nLet’s say we measure the length of the right wing of individual of this species of (angry) bird, it is expected that the wing length will follow a Gaussian distribution with a variance of \\(\\sigma^2\\). We will look at this in more details in the practical exercices later today."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#general-properties-of-distributions",
    "href": "slides/02_5_Distribution/index.html#general-properties-of-distributions",
    "title": "Probability Distribution",
    "section": "General properties of distributions",
    "text": "General properties of distributions\nIn R, there are 4 functions associated to every distribution. As an example, for the Gaussian distribution, they are\n\nrnorm\ndnorm\npnorm\nqnorm\n\n. . .\nKnowing what these functions do will be very useful for this course"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#rnorm",
    "href": "slides/02_5_Distribution/index.html#rnorm",
    "title": "Probability Distribution",
    "section": "rnorm",
    "text": "rnorm\n\nThe r in rnorm is for random\n\n. . .\n\nThis function allows us to randomly sample directly from the distribution of interest.\n\n. . .\nExample\n\n\n\n\n\n\nLet’s assume that we look at the historical record and gather the minimum temperature measured on today’s date for the past 40 years here in Jouvence. Let’s assume that the random sample of 40 temperature measurements has an average of \\(4^{\\circ}C\\) and a standard deviation of \\(2^{\\circ}C\\). We can simulate these values as follow\n\n\nrnorm(40, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#rnorm40-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#rnorm40-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "rnorm(40, mean = 4, sd = 2)",
    "text": "rnorm(40, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#dnorm",
    "href": "slides/02_5_Distribution/index.html#dnorm",
    "title": "Probability Distribution",
    "section": "dnorm",
    "text": "dnorm\n\nThe d in dnorm is for density\n\n. . .\n\nThis function gives the height of the distribution for a chosen value.\n\n. . .\nExample\n\n\n\n\n\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), we can calculate that the likeliness that a temperature of \\(6^{\\circ}C\\) to occur is\n\n\ndnorm(6, mean = 4, sd = 2)\n\n[1] 0.1209854"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#dnorm6-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#dnorm6-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "dnorm(6, mean = 4, sd = 2)",
    "text": "dnorm(6, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#qnorm",
    "href": "slides/02_5_Distribution/index.html#qnorm",
    "title": "Probability Distribution",
    "section": "qnorm",
    "text": "qnorm\n\nThe q in qnorm is for quantile\n\n. . .\n\nThis function gives the value of the distribution given a certain density\n\n. . .\nExample\n\n\n\n\n\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), qnorm allows us to calculate the temperature expected to be obtained in the lowest quartile (1/4). It is calculate as\n\n\nqnorm(0.25, mean = 4, sd = 2)\n\n[1] 2.65102"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#qnorm0.25-mean4-sd2",
    "href": "slides/02_5_Distribution/index.html#qnorm0.25-mean4-sd2",
    "title": "Probability Distribution",
    "section": "qnorm(0.25, mean=4, sd=2)",
    "text": "qnorm(0.25, mean=4, sd=2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm",
    "href": "slides/02_5_Distribution/index.html#pnorm",
    "title": "Probability Distribution",
    "section": "pnorm",
    "text": "pnorm\nThe p in pnorm is for probability distribution function\n. . .\nThis function gives the integral (area under the curve) up to a specified value.\n. . .\nThis is particularly useful because it informs us about the probability that an event is likely to occur (of course assuming a it follows a normal distribution)."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm-1",
    "href": "slides/02_5_Distribution/index.html#pnorm-1",
    "title": "Probability Distribution",
    "section": "pnorm",
    "text": "pnorm\nExample\n\n\n\n\n\n\n\n\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), pnorm will tell us that the probability to have a temperature lower or equal to \\(6^{\\circ}C\\). This is calculated as\n\npnorm(6, mean = 4, sd = 2)\n\n[1] 0.8413447"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm6-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#pnorm6-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "pnorm(6, mean = 4, sd = 2)",
    "text": "pnorm(6, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-and-bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-and-bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Binomial and Bernoulli distribution",
    "text": "Binomial and Bernoulli distribution\nAs I mentionned previsouly, these function are available in base R for a large number of distributions."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-3",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-3",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nrbinom\ndbinom\nqbinom\npbinom"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-3",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-3",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nHowever, sometimes we must know a little bit of theory (as I have shown today) to use the right function.\n\nrbinom\ndbinom\nqbinom\npbinom"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-4",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-4",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\nrbinom\ndbinom\nqbinom\npbinom\n\nWith size = 1"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#other-distributions",
    "href": "slides/02_5_Distribution/index.html#other-distributions",
    "title": "Probability Distribution",
    "section": "Other distributions",
    "text": "Other distributions\nStatisticians and biologists have been very, very (!!) creative in proposing new probability distribution for specific problems\n. . .\nIf you want to learn about the diversity of distributions that are out there, take a look at :\nhttps://en.wikipedia.org/wiki/List_of_probability_distributions\n. . .\nMany of them have been implemented in R, either in base R or specialized packages\n. . .\nIf you want to know if you favourite distribution has been implemented in R take a look at\nhttps://cran.r-project.org/web/views/Distributions.html"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html",
    "href": "slides/05_Matrix_algebra_advanced/index.html",
    "title": "Matrix algebra",
    "section": "",
    "text": "\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\] \\[A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#a-general-way-to-write-matrices",
    "href": "slides/05_Matrix_algebra_advanced/index.html#a-general-way-to-write-matrices",
    "title": "Matrix algebra",
    "section": "",
    "text": "\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\] \\[A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#the-transpose-of-a-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#the-transpose-of-a-matrix",
    "title": "Matrix algebra",
    "section": "The transpose of a matrix",
    "text": "The transpose of a matrix\n\n\n\n\\[A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\\[A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n\nt(A)\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#addition-and-substraction",
    "href": "slides/05_Matrix_algebra_advanced/index.html#addition-and-substraction",
    "title": "Matrix algebra",
    "section": "Addition and Substraction",
    "text": "Addition and Substraction\n\\[\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}\\] \\[C_{ij} = A_{ij} \\pm B_{ij}\\]\n\n\\[\\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} +\n            \\begin{bmatrix}\n                2 & 1\\\\\n                4 & -2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                3+2 & 5+1\\\\\n                1+4 & -2-2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                5 & 6\\\\\n                5 & -4\\\\\n            \\end{bmatrix}\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB &lt;- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#multiplying-a-matrix-by-a-scalar",
    "href": "slides/05_Matrix_algebra_advanced/index.html#multiplying-a-matrix-by-a-scalar",
    "title": "Matrix algebra",
    "section": "Multiplying a matrix by a scalar",
    "text": "Multiplying a matrix by a scalar\n\\[\\mathbf{B} = c\\mathbf{A}\\] \\[B_{ij} = cA_{ij}\\]\n\n\\[\n            0.3 \\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} =  \n            \\begin{bmatrix}\n                0.9 & 1.5\\\\\n                0.3 & -0.6\\\\\n            \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc &lt;- 0.3\nc*A \n\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-multiplications-not-divisions",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-multiplications-not-divisions",
    "title": "Matrix algebra",
    "section": "Matrix multiplications (not divisions!)",
    "text": "Matrix multiplications (not divisions!)\n\\[\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\]\n\\[C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}\\]\nRules\nAssociative: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nDistributive: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}\\)\nNot commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#inner-product",
    "href": "slides/05_Matrix_algebra_advanced/index.html#inner-product",
    "title": "Matrix algebra",
    "section": "Inner product",
    "text": "Inner product\n\\[(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j\\]\n\n\\[\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx &lt;- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n\n     [,1]\n[1,]   31\n[2,]   -8"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#identity-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#identity-matrix",
    "title": "Matrix algebra",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix is a square matrix where all values of its diagonal are 0 except the diagonal values which are all 1s.\n\n\n\\[\n\\mathbf{I}=\\begin{bmatrix}\n                1 & 0 & 0\\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\\\\\n        \\end{bmatrix}\n\\]\n\nThe identity matrix is important because\n\\[\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}\\] or\n\\[\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#diagonal-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#diagonal-matrix",
    "title": "Matrix algebra",
    "section": "Diagonal matrix",
    "text": "Diagonal matrix\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n\\[D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}\\]\nAn example\n\n\\[\n\\begin{bmatrix}\n                -1 & 0 & 0\\\\\n                0 & 0 & 0\\\\\n                0 & 0 & 6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#triangular-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#triangular-matrix",
    "title": "Matrix algebra",
    "section": "Triangular matrix",
    "text": "Triangular matrix\n\n\n\nLower triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}\\]\n\n\n\nUpper triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#symmetric-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#symmetric-matrix",
    "title": "Matrix algebra",
    "section": "Symmetric matrix",
    "text": "Symmetric matrix\nThe values on the above and below the diagonal are match so that \\(A = A^t\\)\n\n\\[\n\\begin{bmatrix}\n                3 & 4 & -10\\\\\n                4 & 5 & 7\\\\\n                -10 & 7 & -6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix \\(\\mathbf{A}\\) is defined as \\(\\mathbf{A}^{-1}\\)\nAs such, \\[\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\] In R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv &lt;- solve(A))\n\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n\nA %*% Ainv\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion-1",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion-1",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nInverting a diagonal matrix\n\\[D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition",
    "href": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\n\nThe Cholesky decomposition allows to decompose a matrix in a triangular, which, when multiplied by its transposed will allow us to recover the initial matrix.\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice\nIn math terms the Cholesky decomposition is defined as \\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\] Example\n\\[\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            1 & 5 & 5\\\\\n            1 & 5 & 14\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            1 & 0 & 0\\\\\n            1 & 2 & 0\\\\\n            1 & 2 & 3 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            0 & 2 & 2\\\\\n            0 & 0 & 3 \\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition-1",
    "href": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition-1",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\n\nWhy is it useful ?\nThere are actually two main reasons :\n\nWorking with triangular matrices is computationally more efficient\nIt can be used to rescale matrices and make MCMC algorithms converge more easily"
  },
  {
    "objectID": "topics/04-phylo/index.html",
    "href": "topics/04-phylo/index.html",
    "title": "Phylogeny",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(rstan))\nrstan_options(\"auto_write\" = TRUE)\noptions(mc.cores = parallel::detectCores())"
  },
  {
    "objectID": "topics/04-phylo/index.html#why-a-phylogenetic-regression",
    "href": "topics/04-phylo/index.html#why-a-phylogenetic-regression",
    "title": "Phylogeny",
    "section": "Why a phylogenetic regression?",
    "text": "Why a phylogenetic regression?\nSuppose you have two traits, measured across many different species – say, social group size (Trait X) and brain size (Trait Y). You want to test the hypothesis that bigger social groups mean a bigger brain. However there’s a catch: some of the species are closely related, and others are not. Its entirely possible that any apparent correlation between Trait X and Trait Y comes from random chance: both traits change randomly along evolutionary time. That means that distantly related species have more time to become different to each other, and close relatives have less “time apart” and are therefore less likely to be different in their two traits.\nBecause every kind of cross-species comparison involves a group of species with a phylogenetic structure, “controlling for phylogeny” has become very common in these kinds of studies. Also, because we are usually missing traits for at least some species in our studies, people often use phylogeny as a guide for guessing what trait values are present in the animals that we haven’t measured."
  },
  {
    "objectID": "topics/04-phylo/index.html#recipe-for-phylogeny",
    "href": "topics/04-phylo/index.html#recipe-for-phylogeny",
    "title": "Phylogeny",
    "section": "Recipe for phylogeny",
    "text": "Recipe for phylogeny\nI love the large and flexible toolbox of Bayesian methods because it can be adapted to fit such a huge array of models – virtually all the models that ecologists want to fit! However, there’s a catch: to fit a model using Stan (or something similar) you have to know exactly what model you’re fitting. However, because these regressions are usually fit using custom software, it can be a challenge to dig to find the exact equations being fit!\nUsing the two resources mentioned above, I was able to write down (I hope!) the equation for the model like this:\n\\[\n\\begin{align}\ny_i &= \\bar{y} + \\beta_1 x_i + a_{s[i]} \\\\\na_{s} &\\sim \\text{MVNormal}(0, \\Sigma_a)\\\\\n\\Sigma_a &= \\begin{bmatrix}\n\\sigma_a^2 & \\lambda_a \\cdot \\sigma_{12} & \\cdots & \\lambda_a \\cdot \\sigma_{1,s} \\\\\n\\lambda_a \\cdot \\sigma_{21} & \\sigma_a^2 & \\cdots & \\lambda_a \\cdot \\sigma_{2,s} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda_a \\cdot \\sigma_{s,1} & \\lambda_a \\cdot \\sigma_{s,2} & \\cdots & \\sigma_a^2\n\\end{bmatrix} \\\\\nx_i &= \\bar{x} + b_{s[i]} \\\\\nb_{s} &\\sim \\text{MVNormal}(0, \\Sigma_b)\\\\\n\\Sigma_b &= \\begin{bmatrix}\n\\sigma_b^2 & \\lambda_b \\cdot \\sigma_{12} & \\cdots & \\lambda_b \\cdot \\sigma_{1,s} \\\\\n\\lambda_b \\cdot \\sigma_{21} & \\sigma_b^2 & \\cdots & \\lambda_b \\cdot \\sigma_{2,s} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda_b \\cdot \\sigma_{s,1} & \\lambda_b \\cdot \\sigma_{s,2} & \\cdots & \\sigma_b^2\n\\end{bmatrix}\n\\end{align}\n\\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\nYou can see that there is no likelihood for the \\(y_i\\) and \\(x_i\\) values. That’s because I’m starting from a simple case where we know the true values for each species. The only thing to estimate is how variable these traits are among species, and how much of that variation correlates with phylogeny. Later I’ll show an example that is closer to real life.\n\n\nYou can see that there are two big variance-covariance matrices here, for the effects of phylogeny on \\(y\\) and \\(x\\). These covariance matrices have three ingredients that are all put together:\n\nthe base How far apart are species on the phylogeny? Many ecologists work with trees where all the tips end at the present day – so all species have the same amount of time going back to their last common ancestor. For trees like this, the diagonal is 1 (i.e., 100% of the evolutionary time). The off-diagonals are the proportion of this total time which is shared between species.\nthe flavour This is a model of species averages. If there were no effect of phylogeny at all, we would still expect species to be a little different. But how different are species from each other? That is controlled by a standard deviation, \\(\\sigma\\), which we multiply the whole matrix by to scale it.\nthe secret sauce The off-diagnal elements of \\(\\Sigma\\) are multiplied by another number between 0 and 1: this is “Pagel’s Lambda”. It acts like a tuning knob, adjusting the amount of phylogenetic flavour that makes it into the model. When \\(\\lambda\\) is 1, we have the maximum amount of covariance coming from the phylogeny. When \\(\\lambda\\) is 0, we are back to an identity matrix and the species are independent.\n\nThere’s another way to write this equation that makes these three parts more clear to see. First we have to make \\(V_{phy}\\), which is the phylogenetic variance-covariance matrix. This has variances and covariances for each species on our tree. For example, for 3 species the phylogenetic variance covariance matrix is:\n\\[\nV_{phy} = \\begin{bmatrix}\n\\sigma_1^2 & \\sigma_{12} & \\sigma_{1,3} \\\\\n\\sigma_{2,1} & \\sigma_2^2 & \\sigma_{2,3} \\\\\n\\sigma_{3,1} & \\sigma_{3,2} & \\sigma_3^2\n\\end{bmatrix}\n\\] The covariances are equal to the proportion of the tree that is shared between two species. The diagonal is the amount of time between the tree’s start and each species. This means that, for a tree where all the tips end at the present day, the diagonal is 1 and the off-diagonal is between 0 and 1.\nThen, we can write the expression for \\(\\Sigma\\) like this:\n\\[\n\\Sigma = \\sigma^2 \\lambda V_{phy} + \\sigma^2 (1 - \\lambda) \\mathbf{I}\n\\] This is equation 4 in @pearse.\nI can rewrite Equation 1 in this style:\n\\[\n\\begin{align}\ny_i &= \\bar{y} + \\beta_1 x_i + a_{s[i]} \\\\\na_{s} &\\sim \\text{MVNormal}(0, \\Sigma_a)\\\\\n\\Sigma_a &= \\sigma_a^2 \\lambda_a V_{phy} + \\sigma_a^2 (1 - \\lambda_a) \\mathbf{I} \\\\\nx_i &= \\bar{x} + b_{s[i]} \\\\\nb_{s} &\\sim \\text{MVNormal}(0, \\Sigma_b)\\\\\n\\Sigma_b &= \\sigma_b^2 \\lambda_b V_{phy} + \\sigma_b^2 (1 - \\lambda_b) \\mathbf{I} \\\\\n\\end{align}\n\\tag{2}\\]\nYou can see I’m using two different trait variances (\\(\\sigma_a\\) and \\(\\sigma_b\\)) and two different amounts of phylogenetic signal (\\(\\lambda_a\\) and \\(\\lambda_b\\)), one for each trait."
  },
  {
    "objectID": "topics/04-phylo/index.html#data-simulation",
    "href": "topics/04-phylo/index.html#data-simulation",
    "title": "Phylogeny",
    "section": "Data simulation",
    "text": "Data simulation\nHere is simulation code from @ives , which generates a dataset where there is a signal for phylogeny and also a relationship between two traits of interest. I’ll use this code to generate a dataset and then estimate the known parameters with a Stan model:\n## simulate data\nset.seed(1618)\nn &lt;- 20\nb0 &lt;- 0\nb1 &lt;- 0\nlambda_x &lt;- .98\nlambda_y &lt;- .8\nsigma_y &lt;- .2\nsigma_x &lt;- .2\n\nphy &lt;- ape::compute.brlen(\n  ape::rtree(n=n),\n  method = \"Grafen\",\n  power = 1)\n\nplot(phy)\nphy.x &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lambda_x))$tree\n\nphy.e &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lambda_y))$tree\n\nx &lt;- ape::rTraitCont(phy.x, model = \"BM\", sigma = sigma_x)\ne &lt;- ape::rTraitCont(phy.e, model = \"BM\", sigma = sigma_y)\nx &lt;- x[match(names(e), names(x))]\nY &lt;- b0 + b1 * x + e\nY &lt;- array(Y)\nrownames(Y) &lt;- phy$tip.label\n\nplot(x, Y)\n\n\n\n\n\n\na simulated phylogeny\n\n\n\n\n\n\n\nsimulated data, with both a phylogenetic signal and a causal relationship between trait X and trait Y.\n\n\n\n\n\nHere’s a simple Stan program which fits the model in Equation 2 to these simulated data.\n\nphylo &lt;- stan_model(here::here(\"topics/04-phylo/phylo.stan\"))\n\nphylo\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int n;\n  int s;\n  vector[n] x;\n  vector[n] y;\n  matrix[s, s] phyvcv;\n}\nparameters {\n  real b0;\n  real b1;\n  real sigma_x;\n  real sigma_y;\n  real logit_lambda_x;\n  real logit_lambda_y;\n}\ntransformed parameters {\n  real&lt;lower=0,upper=1&gt; lambda_x;\n  lambda_x = inv_logit(logit_lambda_x);\n  // y\n  real&lt;lower=0,upper=1&gt; lambda_y;\n  lambda_y = inv_logit(logit_lambda_y);\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(.5, .5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  logit_lambda_x ~ normal(3, .2);\n  logit_lambda_y ~ normal(0, .2);\n  matrix[s, s] vcv_x;\n  vcv_x = add_diag(sigma_x^2*lambda_x*phyvcv, sigma_x^2*(1 - lambda_x));\n  matrix[s, s] vcv_y;\n  vcv_y = add_diag(sigma_y^2*lambda_y*phyvcv, sigma_y^2*(1 - lambda_y));\n  x ~ multi_normal(rep_vector(0, n), vcv_x);\n  y ~ multi_normal(b0 + b1*x, vcv_y);\n} \n\n\nNow we’ll sample the model and plot the posterior distribution of some parameters against the truth:\n\nphylo_sample &lt;- sampling(\n  phylo,\n  data = list(\n    n = n,\n    s = n,\n    x = x,\n    y = Y,\n    phyvcv = ape::vcv(phy)\n  ),\n  chains = 4,\n  refresh = 0)\n\n\nmake_rvar_df &lt;- function(post_draws){\n  post_draws |&gt; \n    posterior::as_draws_rvars() |&gt; \n    # list any parameter that isn't a scalar\n    map_if(\\(x) length(x)&gt;1, list) |&gt; \n    tibble::as_tibble()\n}\n\n\nplot_true_post &lt;- function(truth_df, post_draws_df){\n\n  true_post_df &lt;- truth_df |&gt; \n    left_join(post_draws_df, by = \"name\")\n  \n  true_post_df |&gt; \n    ggplot(aes(y = name, dist = posterior))+ \n    tidybayes::stat_dist_slab() + \n    geom_vline(aes(xintercept = value)) + \n    facet_wrap(~name, scales=\"free\")\n}\n\n\ntruth &lt;- data.frame(sigma_x, sigma_y, b0, \n           b1, lambda_x, lambda_y) |&gt; \n  pivot_longer(cols = everything())\n\n\nposterior_dist_long &lt;- make_rvar_df(phylo_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = posterior_dist_long)\n\n\n\n\n\n\n\n\nWe can see that, at least for these values, parameter recovery isn’t bad, especially for the coefficients \\(\\beta_0\\) and \\(\\beta_1\\). However, at least in this simulation, the parameters describing the phylogenetic signal are all underestimated."
  },
  {
    "objectID": "topics/04-phylo/index.html#tips-from-the-forum",
    "href": "topics/04-phylo/index.html#tips-from-the-forum",
    "title": "Phylogeny",
    "section": "Tips from the forum",
    "text": "Tips from the forum\nI posted about this model in the Stan Discourse forum and I had the good luck to get feedback from Bob Carpenter! Here is the model after including those suggested changes:\n\nphylo_forum &lt;- stan_model(here::here(\"topics/04-phylo/phylo_forum.stan\"))\n\nphylo_forum\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; s;\n  vector[n] x;\n  vector[n] y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[n] zero_vec = rep_vector(0, n);\n}\nparameters {\n  real b0;\n  real&lt;offset=0.5, multiplier=0.5&gt; b1;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;offset=3, multiplier=0.2&gt; logit_lambda_x;\n  real&lt;multiplier=0.2&gt; logit_lambda_y;\n}\ntransformed parameters {\n  real&lt;lower=0, upper=1&gt; lambda_x = inv_logit(logit_lambda_x);\n  real&lt;lower=0, upper=1&gt; lambda_y = inv_logit(logit_lambda_y);\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(0.5, 0.5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  logit_lambda_x ~ normal(3, .2);\n  logit_lambda_y ~ normal(0, .2);\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n  x ~ multi_normal(zero_vec, vcv_x);\n  y ~ multi_normal(b0 + b1 * x, vcv_y);\n} \n\n\n\nphylo_forum_sample &lt;- sampling(phylo_forum,\n                               data = list(\n                                 n = n,\n                                 s = n,\n                                 x = x,\n                                 y = Y,\n                                 phyvcv = ape::vcv(phy)\n                               ), chains = 4, refresh = 0)\n\n\ntruth &lt;- data.frame(sigma_x, sigma_y, b0, \n                    b1, lambda_x, lambda_y) |&gt; \n  pivot_longer(cols = everything())\n\n\nphylo_forum_sample_long &lt;- make_rvar_df(phylo_forum_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = phylo_forum_sample_long)\n\n\n\n\n\n\n\n\nWe get get pretty similar results to the above!\nand an even simpler strategy, replacing the lambda parameter on the logit scale with a beta:\n\nphylo_beta &lt;- stan_model(here::here(\"topics/04-phylo/phylo_beta.stan\"))\n\nphylo_beta\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; s;\n  vector[n] x;\n  vector[n] y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[n] zero_vec = rep_vector(0, n);\n}\nparameters {\n  real b0;\n  real&lt;offset=0.5, multiplier=0.5&gt; b1;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;lower=0,upper=1&gt; lambda_x;\n  real&lt;lower=0,upper=1&gt; lambda_y;\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(0.5, 0.5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  lambda_x ~ beta(9, 1);\n  lambda_y ~ beta(5, 5);\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n  x ~ multi_normal(zero_vec, vcv_x);\n  y ~ multi_normal(b0 + b1 * x, vcv_y);\n} \n\n\n\nphylo_beta_sample &lt;- sampling(phylo_beta,\n                              data = list(\n                                n = n,\n                                s = n,\n                                x = x,\n                                y = Y,\n                                phyvcv = ape::vcv(phy)\n                              ), chains = 4, refresh = 0)\n\nphylo_beta_sample\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nb0        0.09    0.00 0.07 -0.07  0.04  0.09  0.13  0.23  3077    1\nb1        0.06    0.00 0.24 -0.39 -0.09  0.06  0.22  0.54  3812    1\nsigma_x   0.19    0.00 0.04  0.13  0.17  0.19  0.22  0.29  3931    1\nsigma_y   0.17    0.00 0.03  0.12  0.14  0.16  0.18  0.24  2470    1\nlambda_x  0.92    0.00 0.06  0.77  0.89  0.93  0.97  1.00  4515    1\nlambda_y  0.54    0.00 0.13  0.29  0.45  0.55  0.64  0.79  3322    1\nlp__     48.05    0.05 1.89 43.45 47.01 48.43 49.44 50.69  1676    1\n\nSamples were drawn using NUTS(diag_e) at Fri May  9 10:39:39 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nphylo_beta_sample_long &lt;- make_rvar_df(phylo_beta_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = phylo_beta_sample_long)"
  },
  {
    "objectID": "topics/04-phylo/index.html#repeated-sampling-of-these-traits",
    "href": "topics/04-phylo/index.html#repeated-sampling-of-these-traits",
    "title": "Phylogeny",
    "section": "Repeated sampling of these traits",
    "text": "Repeated sampling of these traits\nThe simulation above is giving species means. However in our study we have more than one measurement per species. Measurements of “Trait X” and “Trait Y” are measured on different individuals. In fact, are coming from two completely different datasets! Of course, in the real-world application there will be all kinds of quirky differences between the two datasets: different amounts of effort per species and different species measured in each dataset.\n\nsuppressPackageStartupMessages(library(ape))\n\nset.seed(1618)\n\n# set true parameter values\nn &lt;- 20\nb0_x &lt;- 4\nb0_y &lt;- .5\nb_xy &lt;- -.1\nlam.x &lt;- .98\nlam.e &lt;- .5\nsigma_x &lt;- .4\nsigma_y &lt;- .3\n\n\n\n# simulate phylogeny\nphy &lt;- ape::compute.brlen(\n  ape::rtree(n=n),\n  method = \"Grafen\",\n  power = 1.5)\n\nplot(phy)\n\n\n\n\n\n\n\n# get names from this matrix! needs to line up perfectly\nphyvcv &lt;- ape::vcv(phy)\n\ndistmat_names &lt;- dimnames(phyvcv)[[1]]\n\n# observations per species\nn_obs &lt;- 15\n\n\nphy.x &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lam.x))$tree\n\nphy.e &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lam.e))$tree\n\nx &lt;- ape::rTraitCont(phy.x, model = \"BM\", sigma = sigma_x)\ne &lt;- ape::rTraitCont(phy.e, model = \"BM\", sigma = sigma_y)\nx &lt;- x[match(names(e), names(x))]\n\n## calculate Y\nY &lt;- b0_y + b_xy * x + e\n## calculate X\nX &lt;- b0_x + x\n\n# Y &lt;- array(Y)\nnames(Y) &lt;- phy$tip.label\n\nplot(X, Y)\n\n\n\n\n\n\n\nobs_xy_df &lt;- tibble(X, Y, sp_name = names(x)) |&gt; \n  mutate(\n    sp_id = as.numeric(\n      factor(sp_name, \n             levels = distmat_names))) |&gt; \n  rowwise() |&gt; \n  mutate(obs_x = list(\n    rnorm(n_obs, mean = X, sd = .3)),\n    obs_y = list(rnorm(n_obs, mean = Y, sd = .3)))\n\n\nx_obs_df &lt;- obs_xy_df |&gt; \n  select(sp_id, obs_x) |&gt; unnest(obs_x)\n\n\ny_obs_df &lt;- obs_xy_df |&gt; \n  select(sp_id, obs_y) |&gt; unnest(obs_y)\n\nfit a model that is ready for replication per species:\n\nphylo_obs_cen &lt;- stan_model(here::here(\"topics/04-phylo/phylo_obs_cen.stan\"))\n\nphylo_obs_cen\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int&lt;lower=0&gt; s;\n  // x trait\n  int&lt;lower=0&gt; n_x;\n  vector[n_x] x_obs;\n  array[n_x] int&lt;lower=1,upper=s&gt; sp_id_x;\n  // y trait\n  int&lt;lower=0&gt; n_y;\n  vector[n_y] y_obs;\n  array[n_y] int&lt;lower=1,upper=s&gt; sp_id_y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[s] zero_vec = rep_vector(0, s);\n}\nparameters {\n  real&lt;offset=2,multiplier=2&gt; b0_x;\n  real&lt;offset=.5,multiplier=.8&gt; b0_y;\n  real&lt;offset=0.5, multiplier=0.5&gt; b_xy;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;lower=0, upper=1&gt; lambda_x;\n  real&lt;lower=0, upper=1&gt; lambda_y;\n  vector[s] x_avg;\n  vector[s] y_avg;\n  real&lt;lower=0&gt; sigma_x_obs;\n  real&lt;lower=0&gt; sigma_y_obs;\n}\nmodel {\n  b0_x ~ normal(2, 2);\n  b0_y ~ normal(.5, .8);\n  b_xy ~ normal(0, .2);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  lambda_x ~ beta(9, 1);\n  lambda_y ~ beta(5, 5);\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n  sigma_x_obs ~ exponential(1);\n  sigma_y_obs ~ exponential(1);\n  // species averages\n  x_avg ~ multi_normal(zero_vec, vcv_x);\n  y_avg ~ multi_normal(b_xy * x_avg, vcv_y);\n  // observations of these\n  x_obs ~ normal(b0_x + x_avg[sp_id_x], sigma_x_obs);\n  y_obs ~ normal(b0_y + y_avg[sp_id_y], sigma_y_obs);\n} \n\n\nSampling the model – this produces some warnings that are safe to ignore at this point.\n\nphylo_obs_cen_samp &lt;- sampling(\n  phylo_obs_cen,\n  data = list(\n    s = n,\n    # trait x\n    n_x = nrow(x_obs_df),\n    x_obs = x_obs_df$obs_x,\n    sp_id_x = x_obs_df$sp_id,\n    # trait y\n    n_y = nrow(y_obs_df),\n    y_obs = y_obs_df$obs_y,\n    sp_id_y = y_obs_df$sp_id,\n    # phylogeny\n    phyvcv = phyvcv\n  ), chains = 4, refresh = 0)\n\nsummary(phylo_obs_cen_samp, pars = c(\n  \"b0_x\", \"b0_y\", \"b_xy\", \"sigma_x\", \"sigma_y\", \"lambda_x\", \"lambda_y\", \"sigma_x_obs\", \"sigma_y_obs\"\n))$summary |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse_mean\nsd\n2.5%\n25%\n50%\n75%\n97.5%\nn_eff\nRhat\n\n\n\n\nb0_x\n4.0693266\n0.0138305\n0.2217997\n3.6323760\n3.9324774\n4.0706195\n4.2006340\n4.5227853\n257.1854\n1.0163067\n\n\nb0_y\n0.6253139\n0.0066883\n0.1274193\n0.3690621\n0.5464214\n0.6282446\n0.7039769\n0.8855784\n362.9433\n1.0030129\n\n\nb_xy\n-0.0452080\n0.0035272\n0.1594362\n-0.3521785\n-0.1524897\n-0.0488411\n0.0600449\n0.2758275\n2043.1930\n1.0011012\n\n\nsigma_x\n0.3695900\n0.0025769\n0.0882711\n0.2353304\n0.3039997\n0.3574646\n0.4174324\n0.5784547\n1173.3888\n1.0048087\n\n\nsigma_y\n0.2871955\n0.0017692\n0.0658102\n0.1871795\n0.2398635\n0.2779987\n0.3230354\n0.4432746\n1383.6008\n1.0030746\n\n\nlambda_x\n0.9736224\n0.0006296\n0.0294760\n0.8909818\n0.9645899\n0.9837571\n0.9932469\n0.9994326\n2191.9553\n0.9998153\n\n\nlambda_y\n0.4651460\n0.0034186\n0.1398161\n0.2059164\n0.3619897\n0.4615985\n0.5648397\n0.7342901\n1672.6891\n1.0003333\n\n\nsigma_x_obs\n0.2894070\n0.0002568\n0.0123832\n0.2669616\n0.2808436\n0.2887867\n0.2973855\n0.3143811\n2325.6297\n0.9995239\n\n\nsigma_y_obs\n0.3130218\n0.0002698\n0.0134496\n0.2874380\n0.3037874\n0.3124027\n0.3220685\n0.3407518\n2484.8893\n0.9995594\n\n\n\n\n\n\ntruth_df &lt;- tribble(\n  ~name, ~value,\n  \"b0_x\", b0_x,\n  \"b0_y\", b0_y,\n  \"b_xy\", b_xy,\n  \"sigma_x\", sigma_x,\n  \"sigma_y\", sigma_y,\n  \"lambda_x\", lambda_x,\n  \"lambda_y\", lambda_y\n)\n\nphylo_obs_cen_samp_long &lt;- make_rvar_df(phylo_obs_cen_samp) |&gt; \n  select(-x_avg, -y_avg) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth_df = truth_df, \n               post_draws_df = phylo_obs_cen_samp_long)\n\n\n\n\n\n\n\n\nspecies averages\n\nrvar_list &lt;- phylo_obs_cen_samp |&gt; posterior::as_draws_rvars() |&gt; \n  pluck(\"b0_x\")\n\n\nx_avg_post_long &lt;- make_rvar_df(phylo_obs_cen_samp) |&gt; \n  # calculate averages\n  select(x_avg) |&gt; \n  unnest(x_avg) |&gt; \n  rownames_to_column(var = \"sp_id\") |&gt; \n  mutate(sp_id = readr::parse_number(sp_id),\n         x_total_avg = rvar_list + x_avg )\n\n  \n  \nobs_xy_df |&gt; \n  left_join(x_avg_post_long) |&gt; \n  ggplot(aes(x = sp_name, dist = x_total_avg))+ \n  tidybayes::stat_dist_slab() + \n    geom_point(aes(x = sp_name, y = X))\n\nJoining with `by = join_by(sp_id)`\n\n\n\n\n\n\n\n\n\n\nMissing data\nMany people use phylogenetic information to help when a dataset is missing a lot of traits.\nHere I’m using the same model as above but imagining that a few species are never measured for trait X, but are measured for trait y. There’s also phylogenetic information on both traits.\nNotice that there’s no need to rewrite the model for this! all I need to do is take out some observations from the dataset:\n\n# remove some from the output\n\nabsent_sp &lt;- sample(x_obs_df$sp_id |&gt; unique(), size = 7, replace = FALSE)\n  \nx_obs_NA_df &lt;- x_obs_df |&gt; \n  filter(!(sp_id %in% absent_sp))\n\n\nphylo_obs_NA_samp &lt;- sampling(\n  phylo_obs_cen,\n  data = list(\n    s = n,\n    # trait x\n    n_x = nrow(x_obs_NA_df),\n    x_obs = x_obs_NA_df$obs_x,\n    sp_id_x = x_obs_NA_df$sp_id,\n    # trait y\n    n_y = nrow(y_obs_df),\n    y_obs = y_obs_df$obs_y,\n    sp_id_y = y_obs_df$sp_id,\n    # phylogeny\n    phyvcv = phyvcv\n  ), chains = 4, refresh = 0)\n\n\nrvar_list &lt;- phylo_obs_NA_samp |&gt; \n  posterior::as_draws_rvars() |&gt; \n  pluck(\"b0_x\")\n\n\nx_avg_post_long &lt;- make_rvar_df(phylo_obs_NA_samp) |&gt; \n  # calculate averages\n  select(x_avg) |&gt; \n  unnest(x_avg) |&gt; \n  rownames_to_column(var = \"sp_id\") |&gt; \n  mutate(sp_id = readr::parse_number(sp_id),\n         x_total_avg = rvar_list + x_avg )\n\n  \n  \nobs_xy_df |&gt; \n  left_join(x_avg_post_long) |&gt; \n  mutate(absent = sp_id %in% absent_sp) |&gt; \n  ggplot(aes(x = sp_name, dist = x_total_avg))+ \n  tidybayes::stat_dist_slab() + \n    geom_point(aes(x = sp_name, y = X, col = absent))\n\n\n\n\n\n\n\n## scalar parameters\nphylo_obs_NA_samp_long &lt;- make_rvar_df(phylo_obs_NA_samp) |&gt; \n  select(-x_avg, -y_avg) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth_df = truth_df, \n               post_draws_df = phylo_obs_NA_samp_long)\n\n\n\n\n\n\n\n\nThe model estimates latent parameters for species averages, which are then measured with error. This makes it easy to model unmeasured values. In Bayesian inference, unmeasured quantities are all treated the same, and called “parameters”. So here, we’re modelling all species averages as latent parameters, and saying that most, but not all, actually get measured. The result is posterior samples, not only for slopes and other values of interest, but also for the unmeasured species averages.\nYou can see that the distributions are much flatter for these unmeasured species averages, compared to those that were measured. However, you can also see that the unmeasured averages are moving around, influenced by information coming from Pagel’s Lambda and the other parameters of the model as well."
  },
  {
    "objectID": "topics/02_regression/index.html",
    "href": "topics/02_regression/index.html",
    "title": "Univariate regression",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))"
  },
  {
    "objectID": "topics/02_regression/index.html#load-packages-and-data",
    "href": "topics/02_regression/index.html#load-packages-and-data",
    "title": "Univariate regression",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))"
  },
  {
    "objectID": "topics/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "href": "topics/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "title": "Univariate regression",
    "section": "Statistical models of Penguin bill morphology.",
    "text": "Statistical models of Penguin bill morphology.\nWe’ll be studying the relationship between two numbers about penguin bills. Specifically, we’ll ask “Are longer bills also deeper?”. This question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\nLet’s begin with plotting the data:\n\npenguins |&gt; \n  ggplot(aes(x = bill_len, y = bill_dep)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nBill depth (mm) as predicted by bill length (mm) across the entire palmerpenguins dataset.\n\n\n\n\nLet’s write a simple statistical model for these data:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n\\]\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n\n\n\n\n\nWHERE IS ZERO??\n\n\n\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n\n\nIf we fit a model like this without thinking about the location of zero, we get some pretty silly answers:\n\ncoef(lm(bill_dep ~ bill_len, data = penguins))\n\n(Intercept)    bill_len \n20.88546832 -0.08502128 \n\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\\[\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n\\]\nBut, if we take the data as we found it, we’re going to be talking about \\(\\beta_0\\) as the depth of a penguin’s bill when the bill has 0 length! Clearly that isn’t a very meaningful value. From the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\nA very common choice is to subtract the average from your independent variable, so that penguins with an average bill length now have an average of 0:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n\\]\nNow \\(\\beta_0\\) means the average bill depth at the average bill length. It becomes easier to think about priors:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc."
  },
  {
    "objectID": "topics/02_regression/index.html#prior-predictive-simulations",
    "href": "topics/02_regression/index.html#prior-predictive-simulations",
    "title": "Univariate regression",
    "section": "Prior predictive simulations",
    "text": "Prior predictive simulations\nArmed with this model, it becomes much easier to think about prior predictions.\nWe’ll make a bunch of lines implied by the equation above. There’s two steps:\n\nCenter the predictor\nMake up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\nbill_len_centered &lt;- with(penguins,\n                          bill_len - mean(bill_len,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths &lt;- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n\n\n\n\n\n\n\nShortcuts to these common tasks\n\n\n\nThese tasks are so common that they are automated in helper functions.\nFor centering predictors, see the base R function ?scale (however, doing this by hand is often more convenient)\nFor creating a short vector over the range of a predictor, see modelr::seq_range. The R package modelr has many different functions to help with modelling.\n\n\nTo simulate, we’ll use some matrix algebra, as we saw in lecture:\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nknitr::kable(head(X))\n\n\n\n\n\nsome_bill_lengths\n\n\n\n\n1\n-11.8219298\n\n\n1\n-8.7663743\n\n\n1\n-5.7108187\n\n\n1\n-2.6552632\n\n\n1\n0.4002924\n\n\n1\n3.4558480\n\n\n\n\nknitr::kable(head(B))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninters\n18.1597247\n17.4772899\n15.21873\n16.234098\n16.567020\n18.138464\n14.7523552\n\n\nslopes\n-0.4448028\n-0.4265729\n-0.20687\n-0.005596\n0.433877\n-0.499978\n-0.9988436\n\n\n\n\nprior_mus &lt;- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n\n\n\nSimulating Observations\nThere are always at least TWO kinds of predictions we can be thinking about:\n\nPredicted averages. This is often called a “confidence” interval for a regression line.\nPredicted observations. This is often called a “prediction” interval.\n\nWe can use the full model to simulate observations!\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\nsigmas &lt;- rexp(7, rate = 0.3)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nprior_mus &lt;- X %*% B\n\nprior_obs &lt;- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] &lt;- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n\n\n\n\n\n\n\n\nTidyverse style for those who indulge:\n\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |&gt; \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |&gt; \n  rowwise() |&gt; \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |&gt; \n  tidyr::unnest(cols = c(\"x\", \"avg\", \"obs\")) |&gt; \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\nExperiment with priors that are “too narrow” or “too wide”.\nTry a different distribution than the one used\nInstead of bill size, imagine that we are applying this model to YOUR data. What would you change?"
  },
  {
    "objectID": "topics/02_regression/index.html#linear-regression-in-brms",
    "href": "topics/02_regression/index.html#linear-regression-in-brms",
    "title": "Univariate regression",
    "section": "Linear regression in brms",
    "text": "Linear regression in brms\nNow we write some brms code for this model. We’ll begin with a simple model that has no posterior predictions:\n\n## get data ready\npeng_dep_len_df &lt;- penguins |&gt; \n  tidyr::drop_na(bill_dep, bill_len) |&gt; \n  mutate(bill_len_cen = bill_len - mean(bill_len))\n\n# write formula\nnormal_reg_bf &lt;- bf(bill_dep ~ 1 + bill_len_cen, family = gaussian())\n\n## make priors\nget_prior(normal_reg_bf, data = peng_dep_len_df)\n\n                   prior     class         coef group resp dpar nlpar lb ub tag\n                  (flat)         b                                             \n                  (flat)         b bill_len_cen                                \n student_t(3, 17.3, 2.5) Intercept                                             \n    student_t(3, 0, 2.5)     sigma                                     0       \n       source\n      default\n (vectorized)\n      default\n      default\n\nnormal_reg_prior &lt;- c(\n  prior(normal(0, .5), class = \"b\"),\n  prior(normal(17, 2), class = \"Intercept\"),\n  prior(exponential(.5), class = \"sigma\", lb = 0)\n)\n\nnormal_reg_brm &lt;- brm(formula = normal_reg_bf,\n                      prior = normal_reg_prior,\n                      data = peng_dep_len_df,\n                      file = here::here(\"topics/02_regression/normal_regression.rds\"),\n                      file_refit = \"on_change\", refresh = 0L)\n\nget the variable names\n\ntidybayes::get_variables(normal_reg_brm)\n\n [1] \"b_Intercept\"    \"b_bill_len_cen\" \"sigma\"          \"Intercept\"     \n [5] \"lprior\"         \"lp__\"           \"accept_stat__\"  \"stepsize__\"    \n [9] \"treedepth__\"    \"n_leapfrog__\"   \"divergent__\"    \"energy__\"      \n\n\n\nnormal_reg_brm |&gt; \n  bayesplot::mcmc_areas(pars = c(\"b_bill_len_cen\", \"Intercept\", \"sigma\"))\n\n\n\n\n\n\n\nnormal_reg_brm |&gt; \n  bayesplot::mcmc_areas(pars = \"b_bill_len_cen\") + \n  coord_cartesian(xlim = c(-0.16, 0.16))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nDiscussion : Look just at the posterior distribution of the slope right above. Do we have evidence that there’s a relationship between bill length and bill depth?"
  },
  {
    "objectID": "topics/02_regression/index.html#posterior-predictions-in-r",
    "href": "topics/02_regression/index.html#posterior-predictions-in-r",
    "title": "Univariate regression",
    "section": "Posterior predictions in R",
    "text": "Posterior predictions in R\nWe can calculate a posterior prediction line directly in R for these data. I’ll show each step in this workflow separately:\n\nnormal_reg_brm |&gt; \n  tidybayes::spread_rvars(b_bill_len_cen, Intercept, sigma)\n\n# A tibble: 1 × 3\n   b_bill_len_cen  Intercept        sigma\n       &lt;rvar[1d]&gt; &lt;rvar[1d]&gt;   &lt;rvar[1d]&gt;\n1  -0.085 ± 0.019   17 ± 0.1  1.9 ± 0.072\n\n\ntidybayes helps us extract the posterior distribution of the parameters into a convenient object called an rvar. Learn more about tidybayes here and about the rvar datatype here\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\nnormal_reg_predline &lt;- normal_reg_brm |&gt; \n  tidybayes::spread_rvars(b_bill_len_cen, Intercept) |&gt; \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  mutate(mu = Intercept + b_bill_len_cen*x)\n\nnormal_reg_predline\n\n# A tibble: 5 × 4\n   b_bill_len_cen  Intercept     x         mu\n       &lt;rvar[1d]&gt; &lt;rvar[1d]&gt; &lt;dbl&gt; &lt;rvar[1d]&gt;\n1  -0.085 ± 0.019   17 ± 0.1 -15    18 ± 0.31\n2  -0.085 ± 0.019   17 ± 0.1  -7.5  18 ± 0.18\n3  -0.085 ± 0.019   17 ± 0.1   0    17 ± 0.10\n4  -0.085 ± 0.019   17 ± 0.1   7.5  17 ± 0.18\n5  -0.085 ± 0.019   17 ± 0.1  15    16 ± 0.31\n\n\nFinally we’ll plot these:\n\nnormal_reg_predline |&gt; \n  ggplot(aes(x = x, dist = mu)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n\n\n\n\n\n\n\n\n\nUsing posterior draws individually\nThe above workflow makes a nice figure, but perhaps it helps to see the individual lines to understand what is happening here. We can get these with another tidybayes function spread_draws:\n\nnormal_reg_predline_draws &lt;- normal_reg_brm |&gt; \n  tidybayes::spread_draws(b_bill_len_cen, Intercept, ndraws = 12)\n\nknitr::kable(normal_reg_predline_draws)\n\n\n\n\n.chain\n.iteration\n.draw\nb_bill_len_cen\nIntercept\n\n\n\n\n4\n482\n3482\n-0.0882432\n17.21116\n\n\n2\n508\n1508\n-0.0885217\n17.29615\n\n\n2\n495\n1495\n-0.1132187\n17.05900\n\n\n1\n995\n995\n-0.0665854\n17.21139\n\n\n4\n486\n3486\n-0.0996216\n17.11091\n\n\n3\n368\n2368\n-0.0819782\n17.15651\n\n\n2\n560\n1560\n-0.1071510\n17.12442\n\n\n1\n640\n640\n-0.0819936\n17.26048\n\n\n4\n252\n3252\n-0.1015765\n16.97635\n\n\n4\n282\n3282\n-0.1161147\n17.14538\n\n\n4\n166\n3166\n-0.1056779\n17.27641\n\n\n2\n317\n1317\n-0.0963531\n17.14618\n\n\n\n\n\n\nnormal_reg_predline_draws |&gt; \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  mutate(mu = Intercept + b_bill_len_cen*x) |&gt; \n  ggplot(aes(x = x, y = mu, group = .draw)) + \n  geom_line() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)"
  },
  {
    "objectID": "topics/02_regression/index.html#posterior-predicted-observations",
    "href": "topics/02_regression/index.html#posterior-predicted-observations",
    "title": "Univariate regression",
    "section": "Posterior predicted observations",
    "text": "Posterior predicted observations\n\ndata.frame(bill_len_cen = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  tidybayes::add_predicted_rvars(normal_reg_brm) |&gt; \n  ggplot(aes(x = bill_len_cen, ydist = .prediction)) + \n  stat_lineribbon() +\n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1)\n\n\n\n\n\n\n\n\n\nsummary(normal_reg_brm)\n\n Family: gaussian \n  Links: mu = identity \nFormula: bill_dep ~ bill_len_cen \n   Data: peng_dep_len_df (Number of observations: 342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       17.15      0.10    16.96    17.36 1.00     3883     2619\nbill_len_cen    -0.09      0.02    -0.12    -0.05 1.00     3805     2949\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.93      0.07     1.79     2.07 1.00     4058     2799\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nExtend this model to include species. Specifically, let each species have its own value of the intercept. This involves combining this regression example with the previous activity on discrete predictors.\nWhen you’re done, look at the resulting summary of coefficients. What do you notice that’s different?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\n## dataset\nglimpse(peng_dep_len_df)\n\nRows: 342\nColumns: 9\n$ species      &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A…\n$ island       &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, To…\n$ bill_len     &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.…\n$ bill_dep     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.…\n$ flipper_len  &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 180, 18…\n$ body_mass    &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 330…\n$ sex          &lt;fct&gt; male, female, female, female, male, female, male, NA, NA,…\n$ year         &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…\n$ bill_len_cen &lt;dbl&gt; -4.82193, -4.42193, -3.62193, -7.22193, -4.62193, -5.0219…\n\n## formula\nbill_dep_len_sp_bf &lt;- bf(bill_dep ~ 0 + bill_len_cen + species, family = gaussian())\n\nget_prior(bill_dep_len_sp_bf, data = peng_dep_len_df)\n\n                prior class             coef group resp dpar nlpar lb ub tag\n               (flat)     b                                                 \n               (flat)     b     bill_len_cen                                \n               (flat)     b    speciesAdelie                                \n               (flat)     b speciesChinstrap                                \n               (flat)     b    speciesGentoo                                \n student_t(3, 0, 2.5) sigma                                         0       \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n\nbill_dep_len_sp_prior &lt;- c(\n  prior(normal(17, 2), class = \"b\", coef = \"speciesAdelie\"),\n  prior(normal(17, 2), class = \"b\", coef = \"speciesChinstrap\"),\n  prior(normal(17, 2), class = \"b\", coef = \"speciesGentoo\"),\n  prior(normal(0, .5), class = \"b\", coef = \"bill_len_cen\"),\n  prior(exponential(.5), class = \"sigma\", lb = 0)\n)\n\nbill_dep_len_sp_brm &lt;- brm(formula = bill_dep_len_sp_bf, \n                           data = peng_dep_len_df,\n                           prior = bill_dep_len_sp_prior, \n                           file = here::here(\"topics/02_regression/bill_dep_len_sp_brm.rds\"),\n                           file_refit = \"on_change\")\n\n\nbrms::stancode(bill_dep_len_sp_brm)\n\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += normal_lpdf(b[1] | 0, 0.5);\n  lprior += normal_lpdf(b[2] | 17, 2);\n  lprior += normal_lpdf(b[3] | 17, 2);\n  lprior += normal_lpdf(b[4] | 17, 2);\n  lprior += exponential_lpdf(sigma | 0.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += normal_id_glm_lpdf(Y | X, 0, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\n\ntidyr::expand_grid(bill_len_cen = seq(from = -15, to = 15, length.out = 5),\n                   species = unique(peng_dep_len_df$species)) |&gt;\n  tidybayes::add_epred_rvars(bill_dep_len_sp_brm) |&gt;\n  ggplot(aes(x = bill_len_cen, dist = .epred, group = species)) + \n  stat_dist_lineribbon() + \n  facet_wrap(~species) + \n  geom_point(aes(x = bill_len_cen, y = bill_dep), \n             data = peng_dep_len_df, inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\nsummary(bill_dep_len_sp_brm)\n\n Family: gaussian \n  Links: mu = identity \nFormula: bill_dep ~ 0 + bill_len_cen + species \n   Data: peng_dep_len_df (Number of observations: 342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nbill_len_cen         0.20      0.02     0.16     0.23 1.00     1348     1740\nspeciesAdelie       19.36      0.12    19.12    19.59 1.00     1512     2094\nspeciesChinstrap    17.45      0.15    17.17    17.74 1.00     1855     1777\nspeciesGentoo       14.28      0.11    14.07    14.49 1.00     1879     2066\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.96      0.04     0.89     1.03 1.00     3612     2872\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "topics/02_regression/index.html#exercise-5",
    "href": "topics/02_regression/index.html#exercise-5",
    "title": "Univariate regression",
    "section": "Exercise!",
    "text": "Exercise!\n\nWe have one model without species identity as an independent variable, and one which includes species. Look at the difference in \\(\\sigma\\) between these two models. Why did the value change?\nPosterior predictions Compare the model with species identity to the one without it, by performing posterior predictive checks for each of them (e.g. using `pp_check(..., type = \"dens_overlay\")) ) which model do you prefer?"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#content",
    "href": "topics/parameter_resampling/day_1.html#content",
    "title": "Day 1",
    "section": "Content",
    "text": "Content\nregression with discrete predictors\n\nAfternoon practical exercises"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#course-setup-information",
    "href": "topics/parameter_resampling/day_1.html#course-setup-information",
    "title": "Day 1",
    "section": "Course setup information",
    "text": "Course setup information\n\nsite information\nplagiarism"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#simulation",
    "href": "topics/parameter_resampling/day_1.html#simulation",
    "title": "Day 1",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#quantifying-uncertainty",
    "href": "topics/parameter_resampling/day_1.html#quantifying-uncertainty",
    "title": "Day 1",
    "section": "Quantifying uncertainty",
    "text": "Quantifying uncertainty"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#resampling",
    "href": "topics/parameter_resampling/day_1.html#resampling",
    "title": "Day 1",
    "section": "Resampling",
    "text": "Resampling\nIn frequentist models, we can use the variance covariance matrix of parameters to resample new parameters values. This lets us propagate uncertainty from the estimated parameters to the predicted relationship.\nLet’s demonstrate this with one specific mite:\n\nlrug_water &lt;- mite_water |&gt; \n  filter(sp == \"LRUG\")\n\nlrug_glm &lt;- glm(pa ~ water, data = lrug_water, family = \"binomial\")\n\nNow, with our model object, we can create the resampling distribution of the model predicitons:\n\n# Set seed\nset.seed(42) # The answer !\n\n# a sequence along the range of water values in the data\npredVal &lt;- seq(from = min(lrug_water$water),\n               to = max(lrug_water$water),\n               length.out = 30)\n\nn_resamp &lt;- 500\n\n# Result object\nresampModel &lt;- array(NA_real_,\n                   dim = c(length(predVal), n_resamp))\n\n# Resample model parameters and calculate model predictions\nparamMean &lt;- summary(lrug_glm)$coefficients[,1]\nparamCov &lt;- summary(lrug_glm)$cov.unscaled\n\n# Resample model parameters\nparamSmpl &lt;- MASS::mvrnorm(n_resamp, paramMean, paramCov)\n\n# Calculate model predictions using the resampled model parameters\nfor(j in 1:n_resamp){\n  resampModel[,j] &lt;- binomial(link = \"logit\")$linkinv(\n    paramSmpl[j,1] + paramSmpl[j,2] * predVal)\n}\n\n# make a plot of these predictions\nmatplot(predVal, resampModel, type = \"l\", col = \"grey\", lty = 1)\n\n\n\n\n\n\n\n\nIf we want to find some kind of confidence interval for this line, we can take the quantiles of this resampling:\n\nlow &lt;- apply(resampModel, 1, quantile, probs = .015)\nhigh &lt;- apply(resampModel, 1, quantile, probs = .985)\n\n# plot\nwith(lrug_water, plot(pa ~ water, pch = 21, bg = \"lightblue\"))\npolygon(c(predVal,rev(predVal)),\n        c(low,rev(high)), col=\"thistle\", border=NA)\nlines(predVal, \n      predict(lrug_glm, newdata = list(water = predVal), type = \"response\")\n      )\n\n\n\n\n\n\n\n\nWe can also do this in a tidyverse style, if you are more comfortable with that:\n\ntibble(predVal) |&gt; \n  rowwise() |&gt; \n  mutate(intercept = list(paramSmpl[,1]),\n         slope = list(paramSmpl[,2]),\n         prediction = list(intercept + slope*predVal),\n         prediction_probability = list(plogis(prediction)),\n         low  = quantile(prediction_probability, .015),\n         high = quantile(prediction_probability, .985)) |&gt; \n  ggplot(aes(x = predVal, ymin = low, ymax = high)) + \n  geom_ribbon(fill = \"thistle\") + \n  theme_bw() + \n  ylim(c(0,1))"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#bayesian-approach",
    "href": "topics/parameter_resampling/day_1.html#bayesian-approach",
    "title": "Day 1",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nhere is a simple bayesian model to generate the same inference:\n\\[\n\\begin{align}\ny &\\sim \\text{Bernoulli}(p)\\\\\n\\text{logit}(p) &= \\alpha + X\\beta\\\\\n\\alpha &\\sim \\text{Normal}(-2.5, .5)\\\\\n\\beta &\\sim \\text{Normal}(0, .5)\\\\\n\\end{align}\n\\]\nnormally we would go through a careful process of checking our priors here. At this time we won’t because the point here is to show how the bayesian posterior includes uncertainty, not to demonstrate a full Bayes workflow.\nFirst we compile the model, then we’ll look at the Stan code:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.9.0.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\nlogistic_glm_stan &lt;- cmdstan_model(\n  stan_file = \"topics/parameter_resampling/logistic_bern_logit.stan\", \n  pedantic = TRUE)\n\nlogistic_glm_stan\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  array[n] int&lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  real intercept;\n  real slope;\n}\nmodel {\n  y ~ bernoulli_logit(intercept + slope * x);\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n}\n\n\nHere we see the same three parts of a Stan model that we have reviewed already:\n\ndata\nparameters\nprobability statements\n\nAs you can see, we are using a handy Stan function called bernoulli_logit. This function expects our prediction for the average to be on the logit scale, then applies the logit link function for us.\n\n\nAs a quick review, the logit equation, or inverse-log-odds, is written as \\[\n\\frac{e^\\mu}{1 + e^\\mu}\n\\] Which is also written as\n\\[\n\\frac{1}{1 + e^{-\\mu}}\n\\]\nStan expects our data as a list.\n\nlogistic_glm_stan_samples &lt;- logistic_glm_stan$sample(\n  data = list(n = nrow(lrug_water),\n              y = lrug_water$pa,\n              x = lrug_water$water),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\ncoef(lrug_glm)\n\n(Intercept)       water \n-2.48153943  0.00874349 \n\nlibrary(tidybayes)\n\nspread_rvars(logistic_glm_stan_samples, intercept, slope[]) |&gt; \n  bind_cols(predVal = predVal) |&gt; \n  mutate(pred = posterior::rfun(plogis)(predVal * slope + intercept)) |&gt; \n  ggplot(aes(x = predVal, ydist = pred)) + \n  stat_dist_lineribbon() + \n  guides(fill = \"none\") + \n  ylim(c(0,1))\n\n\n\n\n\n\n\n\n\nAlternative parameterization\nStan contains many functions intended to facilitate writing statistical models. Above, we used the function bernoulli_logit so that we could provide the expression for the average on the logit scale.  Stan also provides an even more efficient function that we can use; it is especially good when we have more than one predictor variable and a vector of slopes:This idea is the core concept of a GLM, or generalized linear model. Statistical distributions have parameters, but for most distributions these have constraints – only some values are “allowed”. For example, the only parameter of a Bernoulli distribution is \\(p\\), the probability of success. We respect this constraint by using a link function: we write an expression for the average of a distribution that can be any real number, and put it through a link function to get the value for \\(p\\).\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE NOTE below you will see the relative path to the stan file (stan/logistic.stan). Immediately below you will see the Stan file content. You can copy and paste this to your own computer!\n\n\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm &lt;- cmdstan_model(stan_file = \"topics/parameter_resampling/logistic.stan\", \n                               pedantic = TRUE)\n\nlogistic_bern_glm\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 1] x;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  real intercept;\n  vector[1] slope;\n}\nmodel {\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n  y ~ bernoulli_logit_glm(x, intercept, slope);\n}"
  },
  {
    "objectID": "topics/correlated_effects/index.html",
    "href": "topics/correlated_effects/index.html",
    "title": "Summarizing many univariate models",
    "section": "",
    "text": "We’ve already looked at univariate models. When we fit the same model to multiple different groups, we don’t expect the same values for all the coefficients. Each thing we are studying will respond to the same variable in different ways.\nHierarchial models represent a way to model this variation, in ways that range from simple to complex.\nBefore we dive in with hierarchical structure, let’s build a bridge between these two approaches.\nThis is useful to help us understand what a hierarchical model does.\nHowever it is also useful from a strict model-building perspective – so useful that Andrew Gelman calls it a “Secret Weapon”"
  },
  {
    "objectID": "topics/correlated_effects/index.html#loading-models-and-data",
    "href": "topics/correlated_effects/index.html#loading-models-and-data",
    "title": "Summarizing many univariate models",
    "section": "Loading models and data",
    "text": "Loading models and data\n\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ggplot2)\nlibrary(tidyr)\n# library(cmdstanr)\nsuppressPackageStartupMessages(library(rstan))\nrstan_options(\"auto_write\" = TRUE)\noptions(mc.cores = parallel::detectCores())\nlibrary(tidybayes)\n\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\n\nAnd some quick data restructuring to combine both.\n\n# combine data and environment\nmite_data_long &lt;- bind_cols(mite.env, mite) |&gt; \n  pivot_longer(Brachy:Trimalc2, names_to = \"spp\", values_to = \"abd\")\n\nTo keep things simple and univariate, let’s consider only water concentration as an independent variable.\nFirst, a quick word about centering and scaling a predictor variable:\n\nI center the predictor by subtracting the mean. This changes the intercept of my linear predictor. it becomes the mean log-odds of occurrance when the water content is average\nI divide water content by 100. The dataset has units of grams per Litre of water (see ?vegan::mite.env for more details). This is fine, but I don’t think mites are able to sense differences as precise as a millimeter of water either way. by dividing by 10 I transform this into centilitres, which is more informative.\n\n\nmite_data_long_transformed &lt;- mite_data_long |&gt; \n  mutate(presabs = as.numeric(abd&gt;0),\n         # center predictors\n         water = (WatrCont - mean(WatrCont)) / 100\n         )\n\nmite_data_long_transformed |&gt; \n  ggplot(aes(x = water, y = presabs)) + \n  geom_point() + \n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) + \n  facet_wrap(~spp)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nsome things to notice about this figure:\n\nthe x-axis scale has been transformed from “grams per litre” to “centilitres away from average\nthere is a ton of variation in how different species respond to water!\n\n\nmite_many_glms &lt;- mite_data_long_transformed |&gt; \n  nest_by(spp) |&gt; \n  mutate(logistic_regressions = list(\n    glm(presabs ~ water,\n        family = \"binomial\",\n        data = data))) |&gt; \n  mutate(coefs = list(broom::tidy(logistic_regressions)))\n\n\n\n\n\n\n\nSplit-Apply-Combine\n\n\n\nTo explore this kind of thinking, we are going to use an approach sometimes called “split-apply-combine”\nThere are many possible ways to do this in practice. We are using a technique here from the tidyverse, which you can read more about.\n\n\n\nmite_many_glm_coefs &lt;- mite_many_glms |&gt; \n  select(-data, -logistic_regressions) |&gt; \n  unnest(coefs)\n\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate, y = spp,\n             xmin = estimate - std.error,\n             xmax = estimate + std.error)) + \n  geom_pointrange() + \n  facet_wrap(~term, scales = \"free\")\n\n\n\n\n\n\n\n\nAs you can see, some of these estimates are high, others low. We could also plot these as histograms to see this distribution.\n\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate)) + \n  geom_histogram(binwidth = .5) + \n  facet_wrap(~term, scales = \"free\")\n\n\n\n\n\n\n\n\nOnce again, the two parameters of this model represent:\n\nIntercept The probability (in log-odds) of a species being present at the average water concentration. some species are common, others are rare.\nwater this is the change in probability (in log-odds) as water increases by one centilitre per litre of substrate."
  },
  {
    "objectID": "topics/correlated_effects/index.html#say-it-in-stan",
    "href": "topics/correlated_effects/index.html#say-it-in-stan",
    "title": "Summarizing many univariate models",
    "section": "Say it in Stan",
    "text": "Say it in Stan\nThe above tidyverse approach is very appealing and intuitive, but we can also do the same procedure in Stan.\n\nall_species_unpooled &lt;- stan_model(\n  file = \"topics/correlated_effects/all_species_unpooled.stan\")\n\nall_species_unpooled\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  // number of rows in dataset\n  int&lt;lower=0&gt; Nsites;\n  // number of species\n  int&lt;lower=0&gt; S;\n  // one environmental variable to use in prediction\n  vector[Nsites] x;\n  // response site (rows) by species (columns) 2D array\n  array[Nsites,S] int &lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  // parameters are now VECTORS\n  vector[S] intercept;\n  vector[S] slope;\n}\nmodel {\n  for (s in 1:S){\n    y[,s] ~ bernoulli_logit(intercept[s] + slope[s] * x);\n  }\n  // priors don't change because Stan is vectorized:\n  // every element of the vector gets the same prior\n  intercept ~ normal(0, 3);\n  slope ~ normal(0, 3);\n} \n\n\nLet’s fit this model by passing in the data:\n\nmite_bin &lt;- mite\nmite_bin[mite_bin&gt;0] &lt;- 1\n\nmite_pa_list &lt;- list(\n      Nsites = nrow(mite_bin),\n      S = ncol(mite_bin),\n      x = with(mite.env, (WatrCont - mean(WatrCont))/100),\n      y = as.matrix(mite_bin)\n    )\n\nall_species_unpooled_posterior &lt;- \n  rstan::sampling(\n    all_species_unpooled,\n    data = mite_pa_list, \n    refresh = 1000, chains = 4\n  )\n\nnow let’s try to plot this:\n\n# start by looking at the names of variables\n# get_variables(all_species_unpooled_posterior)\n\npost_pred &lt;- tidybayes::spread_rvars(all_species_unpooled_posterior, \n             intercept[spp_id], slope[spp_id]) |&gt; \n  expand_grid(water = seq(from = -4, to = 4, length.out = 10)) |&gt; \n  mutate(prob = posterior::rfun(plogis)(intercept + slope*water),\n         spp = colnames(mite_bin)[spp_id]) |&gt; \n  ggplot(aes(x = water, dist = prob)) + \n  tidybayes::stat_lineribbon() + \n  facet_wrap(~spp) + \n  scale_fill_brewer(palette = \"Greens\")\n\npost_pred\n\n\n\n\n\n\n\n\nWe can imitate the original figure by adding the observed data in orange:\n\npost_pred + \n  geom_point(aes(x = water, y = presabs), \n             inherit.aes = FALSE, \n             data = mite_data_long_transformed,\n             pch = 21, \n             fill = \"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\n\nAdd hierarchy to both the slope AND the intercept of this model. Do not try to model them as correlated: the lmer syntax would be y ~ 1 + water + (1 | species) + (0 + water | species)\nMake a plot of the slope coefficients from both models, with and without hierarchy. Are they the same? How are they different?\nMake a posterior prediction of species richness in these communities.\n\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nFirst we rewrite the Stan model from above, replacing the standard deviations with parameters. We do this for the priors on the intercepts and slopes separately.\n\nall_species_partpooled &lt;- stan_model(\n  file = \"topics/correlated_effects/all_species_partpooled.stan\")\n\nall_species_partpooled\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  // number of rows in dataset\n  int&lt;lower=0&gt; Nsites;\n  // number of species\n  int&lt;lower=0&gt; S;\n  // one environmental variable to use in prediction\n  vector[Nsites] x;\n  // response site (rows) by species (columns) 2D array\n  array[Nsites,S] int &lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  // parameters are now VECTORS\n  vector[S] intercept;\n  vector[S] slope;\n  real&lt;lower=0&gt; sigma_intercept;\n  real&lt;lower=0&gt; sigma_slope;\n  real avg_intercept;\n  real avg_slope;\n}\nmodel {\n  for (s in 1:S){\n    y[,s] ~ bernoulli_logit(intercept[s] + slope[s] * x);\n  }\n  // priors don't change because Stan is vectorized:\n  // every element of the vector gets the same prior\n  intercept ~ normal(avg_intercept, sigma_intercept);\n  slope ~ normal(avg_slope, sigma_slope);\n  avg_intercept ~ normal(0, 1);\n  avg_slope ~ normal(0, 1);\n  sigma_intercept ~ exponential(5);\n  sigma_slope ~ exponential(5);\n\n} \n\n\nSample the model\n\nall_species_partpooled_posterior &lt;- \n  sampling(\n    all_species_partpooled,\n    data = mite_pa_list, \n    refresh = 1000, chains = 4\n    )\n\nPlot posterior predictions of trendlines, just as before:\n\npost_pred_partpooled &lt;- tidybayes::spread_rvars(all_species_partpooled_posterior, \n             intercept[spp_id], slope[spp_id]) |&gt; \n  expand_grid(water = seq(from = -4, to = 4, length.out = 10)) |&gt; \n  mutate(prob = posterior::rfun(plogis)(intercept + slope*water),\n         spp = colnames(mite_bin)[spp_id]) |&gt; \n  ggplot(aes(x = water, dist = prob)) + \n  tidybayes::stat_lineribbon() + \n  facet_wrap(~spp) + \n  scale_fill_brewer(palette = \"Reds\") +\n  geom_point(aes(x = water, y = presabs), \n             inherit.aes = FALSE, \n             data = mite_data_long_transformed,\n             pch = 21, \n             fill = \"blue\")\n\npost_pred_partpooled\n\n\n\n\n\n\n\n\n\nComparing slope estimates of both models\n\nunpooled_slopes &lt;- tidybayes::spread_rvars(all_species_unpooled_posterior, \n              slope[spp_id]) |&gt; \n  mutate(type = \"non-hierarchical\")\n\npartpooled_slopes &lt;- tidybayes::spread_rvars(all_species_partpooled_posterior, \n              slope[spp_id]) |&gt; \n  mutate(type = \"hierarchical\")\n\n\npartpooled_slopes |&gt; \n  bind_rows(unpooled_slopes) |&gt; \n  mutate(species = colnames(mite_bin)[spp_id],\n         med_slope = median(slope),\n         species = forcats::fct_reorder(species, med_slope)) |&gt; \n  ggplot(aes(x = species, dist = slope, colour = type)) + \n  stat_pointinterval(position = position_dodge(width = .5)) + \n  # facet_wrap(~type)\n  NULL\n\n\n\n\n\n\n\n\n\n\nPosterior distribution of species richness\nWe can always calculate things out of the posterior distribution, and get a new distribution which reflects the uncertainty in all our parameter estimates.\nHere I’m suggesting we calculate the relationship between species richness and water concentration\n\ntidybayes::spread_draws(all_species_partpooled_posterior, \n             intercept[spp_id], slope[spp_id], ndraws = 700) |&gt; \n  expand_grid(water = seq(from = -8, to = 6, length.out = 10)) |&gt; \n  mutate(prob = plogis(intercept + slope*water),\n         spp = colnames(mite_bin)[spp_id]) |&gt; \n  nest_by(water, .draw) |&gt; \n  mutate(S = sum(data$prob)) |&gt; \n  select(-data) |&gt; unnest() |&gt; \n  ggplot(aes(x = water, y = S)) + \n  tidybayes::stat_lineribbon() + \n  scale_fill_brewer(palette = \"Blues\", direction = -1)\n\nWarning: `cols` is now required when using `unnest()`.\nℹ Please use `cols = c()`."
  },
  {
    "objectID": "slides/03_Stan/index.html",
    "href": "slides/03_Stan/index.html",
    "title": "Stan",
    "section": "",
    "text": "\\[\n\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this",
    "title": "Stan",
    "section": "",
    "text": "\\[\n\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this-1",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this-1",
    "title": "Stan",
    "section": "What is the point of this",
    "text": "What is the point of this\n\\[\n\\begin{equation}\nP(\\boldsymbol{\\theta}|\\text{data}) = \\frac{P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta})}{P(\\text{data})}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this-2",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this-2",
    "title": "Stan",
    "section": "What is the point of this",
    "text": "What is the point of this\n\\[\n\\begin{equation}\nP(\\boldsymbol{\\theta}|\\text{data}) \\propto P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\nA concrete example\n\n\n\nLaid\nHatched\n\n\n\n\nEgg 1\nChick 1\n\n\nEgg 2\nChick 2\n\n\nEgg 3\nChick 3"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-1",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-1",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\nWhat’s the probability of this dataset?\nThis is called the likelihood\n\\[\n\\begin{align}\n\\text{hatch} &\\sim \\text{Binomial}(p, 5) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-2",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-2",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\n\\[\n\\begin{align*}\n    P(3, 4, 5 | p) &= \\text{Binomial}(3 | p, 5) \\\\\n                   &\\quad \\times \\text{Binomial}(4 | p, 5) \\\\\n                   &\\quad \\times \\text{Binomial}(5 | p, 5)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-3",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-3",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\n\\[\n\\begin{align*}\n    \\ln{P(3, 4, 5 | p)} &=  \\log(\\text{Binomial}(3 | p, 5)) \\\\\n                   &+  \\log(\\text{Binomial}(4 | p, 5)) \\\\\n                   &+ \\log(\\text{Binomial}(5 | p, 5))\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#log-likelihood-code-in-r",
    "href": "slides/03_Stan/index.html#log-likelihood-code-in-r",
    "title": "Stan",
    "section": "log-likelihood code in R",
    "text": "log-likelihood code in R\n\nsurv &lt;- c(3, 4, 5)\n\ncalc_ll &lt;- function(x) {\n  res &lt;- sum(-dbinom(surv, 5,\n                     prob = x,\n                     log = TRUE))\n  return(res)\n}\n\nprob_val &lt;- seq(from = 0, to = 1,\n                length.out = 30)\nlog_lik &lt;- numeric(30L)\n\nfor (i in 1:length(prob_val)) {\n  log_lik[i] &lt;- calc_ll(prob_val[i])\n}\npar(cex = 3)\nplot(prob_val, log_lik, type = \"b\")"
  },
  {
    "objectID": "slides/03_Stan/index.html#make-it-bayes-add-a-prior",
    "href": "slides/03_Stan/index.html#make-it-bayes-add-a-prior",
    "title": "Stan",
    "section": "Make it Bayes: add a prior",
    "text": "Make it Bayes: add a prior\n\\[\n\\begin{align}\n\\text{hatch} &\\sim \\text{Binomial}(p, 5) \\\\\np &\\sim \\text{Uniform}(0,1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#make-it-bayes-add-a-prior-1",
    "href": "slides/03_Stan/index.html#make-it-bayes-add-a-prior-1",
    "title": "Stan",
    "section": "Make it Bayes: add a prior",
    "text": "Make it Bayes: add a prior\n\\[\n\\begin{align*}\nP(\\boldsymbol{\\theta}|\\text{data}) &\\propto P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta}) \\\\[10pt]\n&\\propto \\text{Bin}(3|p,5) \\cdot \\text{Bin}(4|p,5) \\\\\n&\\qquad \\cdot \\text{Bin}(5|p,5)\\cdot \\text{Uniform}(p|0,1) \\\\[10pt]\n\\log(P(\\boldsymbol{\\theta}|\\text{data})) &\\propto \\log(\\text{Bin}(3|p,5)) + \\log(\\text{Bin}(4|p,5)) \\\\\n&\\qquad + \\log(\\text{Bin}(5|p,5)) + \\log(\\text{Uniform}(p|0,1)) \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#sampling-the-uncalculable",
    "href": "slides/03_Stan/index.html#sampling-the-uncalculable",
    "title": "Stan",
    "section": "Sampling the uncalculable",
    "text": "Sampling the uncalculable\n\n\n\n\nMANIAC I, 1956 (top) Arianna W. Rosenbluth"
  },
  {
    "objectID": "slides/03_Stan/index.html#sampling-the-uncalculable-1",
    "href": "slides/03_Stan/index.html#sampling-the-uncalculable-1",
    "title": "Stan",
    "section": "Sampling the uncalculable",
    "text": "Sampling the uncalculable"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-stan",
    "href": "slides/03_Stan/index.html#what-is-stan",
    "title": "Stan",
    "section": "What is Stan?",
    "text": "What is Stan?\n\n\n\n\n\n\n\n\n\n\n\nStanisław Ulam"
  },
  {
    "objectID": "slides/03_Stan/index.html#stan",
    "href": "slides/03_Stan/index.html#stan",
    "title": "Stan",
    "section": "Stan",
    "text": "Stan\nhttps://mc-stan.org/\n\nA comprehensive software ecosystem aimed at facilitating the application of Bayesian inference\n\nFull Bayesian statistical inference with MCMC sampling (but not only)\nIntegrated with most data analysis languages (R, Python, MATLAB, Julia, Stata)"
  },
  {
    "objectID": "slides/03_Stan/index.html#why-stan",
    "href": "slides/03_Stan/index.html#why-stan",
    "title": "Stan",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nOpen source\nExtensive documentation\nPowerful sampling algorithm\nLarge and active online community!"
  },
  {
    "objectID": "slides/03_Stan/index.html#hmc",
    "href": "slides/03_Stan/index.html#hmc",
    "title": "Stan",
    "section": "HMC",
    "text": "HMC\n\nMetropolis and Gibbs limitations:\n\nA lot of tuning to find the best spot between large and small steps\nInefficient in high-dimensional spaces\nCan’t travel long distances between isolated local minimums\n\nHamiltonian Monte Carlo:\n\nUses a gradient-based MCMC to reduce the random walk (hence autocorrelation)\nStatic HMC\nNo-U-Turn Sampler (NUTS)\nDon’t get it? Viz it!"
  },
  {
    "objectID": "slides/03_Stan/index.html#how-to-stan",
    "href": "slides/03_Stan/index.html#how-to-stan",
    "title": "Stan",
    "section": "How to Stan",
    "text": "How to Stan"
  },
  {
    "objectID": "slides/03_Stan/index.html#why-to-stan",
    "href": "slides/03_Stan/index.html#why-to-stan",
    "title": "Stan",
    "section": "WHY to Stan",
    "text": "WHY to Stan"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html",
    "href": "slides/07_Convergence_trick/index.html",
    "title": "Convergence trick",
    "section": "",
    "text": "When estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure.\nHowever, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution",
    "title": "Convergence trick",
    "section": "",
    "text": "When estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure.\nHowever, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-1",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-1",
    "title": "Convergence trick",
    "section": "Playing with the Gaussian distribution",
    "text": "Playing with the Gaussian distribution\nEven if you run the model for many, many (many !) iterations, it never seems to converge.\nWhat should we do ?"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-2",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-2",
    "title": "Convergence trick",
    "section": "Playing with the Gaussian distribution",
    "text": "Playing with the Gaussian distribution\nThere is a very cool trick that can help us here.\nBefore we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution (\\(\\mathcal{N}(0,1)\\)) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.\n\nThe convergence trick\nIf we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.\n\nAny ideas how to do this ?"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-1",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-1",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nTranslation"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-2",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-2",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nTranslation\nMathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.\n. . .\nThis means that\n\\[\\mathcal{N}(\\mu, \\sigma^2)\\]\n. . .\nis exactly the same as\n\\[\\mathcal{N}(0, \\sigma^2) + \\mu\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-3",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-3",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nScaling"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-4",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-4",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nScaling\nMathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.\n. . .\nThis means that\n\\[\\mathcal{N}(\\mu, \\sigma^2)\\]\n. . .\nis exactly the same as\n\\[\\mathcal{N}(\\mu, 1) \\times \\sigma^2\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-5",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-5",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nThe convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from outside the distribution\n\\[\\mathcal{N}(0, 1) \\times \\sigma^2 + \\mu\\] When implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-6",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-6",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-7",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-7",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-8",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-8",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution.\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-9",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-9",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution.\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\nTo show how our convergence trick works for a multivariate Gaussian distribution, let’s first visualize the two dimensional version of this distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#bivariate-gaussian-distribution",
    "href": "slides/07_Convergence_trick/index.html#bivariate-gaussian-distribution",
    "title": "Convergence trick",
    "section": "Bivariate Gaussian distribution",
    "text": "Bivariate Gaussian distribution\n\\[\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  0\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n  2 & -1\\\\\n  -1 & 2\\\\\n\\end{bmatrix}\\right)\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-10",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-10",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nMultivariate Gaussian distribution\n\nTranslation\nFor a multivariate distribution, a translation amounts to adding a vector of values to make the translation.\nMathematically, this means that\n\\[\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right)=\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  \\vdots\\\\\n  0\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right) + \\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-11",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-11",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nMultivariate Gaussian distribution\n\nScaling\nUnlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform… But mathematician and statistician have worked hard to figure out how to do this properly.\nHowever, we need to delve a little deeper into matrix algebra to understand how to scale a multivariate Gaussian distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#scaling-a-covariance-matrix",
    "href": "slides/07_Convergence_trick/index.html#scaling-a-covariance-matrix",
    "title": "Convergence trick",
    "section": "Scaling a covariance matrix",
    "text": "Scaling a covariance matrix\n\nFirst recall that a covariance matrix \\(\\mathbf{\\Sigma}\\) is a square matrix (i.e. it is an \\(n\\times n\\) matrix).\nTo scale \\(\\mathbf{\\Sigma}\\), we cannot only multiply it by a scalar or even by a single matrix, we need to use the following matrix multiplication\n\\[\\mathbf{L}\\mathbf{\\Sigma}\\mathbf{L}^t\\] where \\(\\mathbf{L}\\) is a \\(p\\times n\\) matrix of weight to be used for the scaling (a “scaling” matrix) and \\(\\mathbf{L}^t\\) is its tranpose.\nThe technical reason why we need to use the equation above is to ensure that the resulting scaled covariance matrix also has an \\(n \\times n\\) dimension.\nIf only \\[\\mathbf{L}\\mathbf{\\Sigma}\\] is used the dimension of the resulting matrix also would be \\(p \\times p\\)."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\n\nBecause in our problem weighting (or scaling) matrices is usually done with other covariance matrices, to apply the matrix scaling operation described previously, we need to find a way to square-root a matrix.\nThis where the genious of André-Louis Cholesky comes to the rescue."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-1",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-1",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\n\nCholesky decomposition\n\nAndré-Louis Cholesky discovered a matrix decomposition approach probably around 1902 (so when he was 27 years old!), although it was attributed to him a few years after his death.\nThe Cholesky decomposition allows to decompose a square matrix in a triangular matrix, which, when multiplied by its transposed will allow us to recover the initial matrix.\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrices.\nIn math terms the Cholesky decomposition is defined as \\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-2",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-2",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\n\nCholesky decomposition\n\nExample\n\\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\]\n\\[\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            1 & 5 & 5\\\\\n            1 & 5 & 14\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            1 & 0 & 0\\\\\n            1 & 2 & 0\\\\\n            1 & 2 & 3 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            0 & 2 & 2\\\\\n            0 & 0 & 3 \\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-12",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-12",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nMultivariate Gaussian distribution\n\nScaling\n\nTo scale the following multivariate Gaussian distribution \\[\\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{\\Sigma}\\right),\\]\nThe following steps need to be applied\n\nApply the Cholesky decomposition on the scaling matrix, here \\(\\mathbf{\\Sigma}\\) \\[\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^t\\]\nMultiply the \\(\\mathbf{L}\\) matrix to a standard variance multivariate Gaussian distribution\n\n\\[\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t.\\]\nRecall, that \\(\\mathbf{I}\\) is the identity matrix."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-13",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-13",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nMultivariate Gaussian distribution\nIf we apply translation and scaling together on a multivariate Gaussian distribution, we get\n\\[\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\mathbf{0},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t + \\boldsymbol{\\mu}\\] When implementing in Stan some of the models we will discuss in this course, this convergence trick becomes very practical because it can lead a model to convergence much faster than without using this trick."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html",
    "href": "slides/09_Gaussian_process/index.html",
    "title": "Gaussian process",
    "section": "",
    "text": ". . .\nAn already (very !) general formulation\n\nSo far, we have built hierarchical models that can be integrated into the following framework.\n\n. . .\n\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n. . .\n\nwhere\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mu, \\mathbf{\\Sigma})\\]\n\n. . .\n\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is designed matrix of “explanatory” variables with \\(n\\) rows (samples) and \\(q\\) columns\n\\(\\mu\\) is a vector defining the average importance of hierarchical parameters\n\\(\\mathbf{\\Sigma}\\) is a matrix defining the covariance structure of hierarchical parameters"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far",
    "href": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far",
    "title": "Gaussian process",
    "section": "",
    "text": ". . .\nAn already (very !) general formulation\n\nSo far, we have built hierarchical models that can be integrated into the following framework.\n\n. . .\n\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n. . .\n\nwhere\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mu, \\mathbf{\\Sigma})\\]\n\n. . .\n\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is designed matrix of “explanatory” variables with \\(n\\) rows (samples) and \\(q\\) columns\n\\(\\mu\\) is a vector defining the average importance of hierarchical parameters\n\\(\\mathbf{\\Sigma}\\) is a matrix defining the covariance structure of hierarchical parameters"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far-1",
    "href": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far-1",
    "title": "Gaussian process",
    "section": "Hierarchical models so far",
    "text": "Hierarchical models so far\nAn already (very !) general formulation\nAnother way to write this generalized formulation is\n\\[\\mathbf{y}_i = \\mathbf{X}_{ij} \\boldsymbol{\\beta}_j + \\mathbf{Z}_{ik}\\mathbf{b}_{k} + \\boldsymbol{\\varepsilon}_{ij}\\] where\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mu, \\mathbf{\\Sigma})\\] and\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\nSo far we have seen many (!) types of hierarchical models, which got increasingly more complex in their structure.\n. . .\nLet’s continue on that slippery slope…\n. . .\nWould you know how to constraint (spatially, temporally, phylogenetically, etc.) such a model ?"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-1",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-1",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\nIf we get back to a model that has a single hierarchy on the intercept such that \\[\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{b},\\sigma^2)\\]\n. . .\nwhere \\[\\mathbf{b} \\sim \\mathcal{N}(\\mu, \\sigma^2).\\]\n. . .\nIf we want to account for a constraint on the previously presented model, we can rewrite the equation fo \\(\\mathbf{b}\\) as \\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n. . .\nwhere \\(f(d)\\) is a function of a (spatial, temporal, phylogenetic, …) distance matrix"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-2",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-2",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n. . .\nWhat this equation means conceptually is that the variance associated to \\(\\mathbf{b}\\) is not a constant, it changes based on distance (across space, time, phylogeny, etc.). This is known as a Gaussian process."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-bit-of-history",
    "href": "slides/09_Gaussian_process/index.html#a-bit-of-history",
    "title": "Gaussian process",
    "section": "A bit of history",
    "text": "A bit of history\n. . .\nIn statistics, Gaussian processes have a unique history. The development of this type of model is closely linked to the estimation of mineral deposits.\n. . .\nSpatial Gaussian processes are also called geostatistical models, where the prefix geo refers to geology, not geography, as one may be led to believe.\n. . .\nAs mining engineers are at the root of the development of Gaussian processes, the language associated with this type of model is influenced by this field."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-bit-of-history-1",
    "href": "slides/09_Gaussian_process/index.html#a-bit-of-history-1",
    "title": "Gaussian process",
    "section": "A bit of history",
    "text": "A bit of history\nGaussian processes have been developed in the 1950s by\n\n\n\n\n\n\n\n\nDaniel G. Krige (1919–2013)\n\n\n\n\n\n\n\n\nGeorges Mathéron (1930–2000)"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#assumption-with-gaussian-processes",
    "href": "slides/09_Gaussian_process/index.html#assumption-with-gaussian-processes",
    "title": "Gaussian process",
    "section": "Assumption with Gaussian processes",
    "text": "Assumption with Gaussian processes\n. . .\nIn general, when defining a Gaussian process, we make the following assumptions:\n. . .\n\nThe closer two samples are, the more similar they are.\n\n. . .\n\nAfter a certain distance, it is no longer necessary to consider that a sample influences another.\n\n. . .\nNote The distance of influence of a sample on another can be different depending on what is being studied, where it is being studied and when it is being studied"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd",
    "href": "slides/09_Gaussian_process/index.html#fd",
    "title": "Gaussian process",
    "section": "\\(f(d)\\)",
    "text": "\\(f(d)\\)\n. . .\nSo, what does \\(f(d)\\) looks like exactly ?\n. . .\nIn theory, \\(f(d)\\) can be anything…\n. . .\nHowever, in practice, there are particularities of the functions that are defined by the assumptions we impose on our model.\n. . .\nHere is a classic structure these variance function take"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\n. . .\nNugget effect"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-1",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-1",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\nRange"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-2",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-2",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\nSill"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---types-of-functions",
    "href": "slides/09_Gaussian_process/index.html#fd---types-of-functions",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - types of functions",
    "text": "\\(f(d)\\) - types of functions\n. . .\nMany functions that have been proposed have this general shape\n. . .\nLet’s first study the exponential function\n\\[C_0 + C_1 \\left(1-e^{-d/a}\\right)\\]\n. . .\nwhere\n\n\\(C_0\\) is the nugget effect\n\n. . .\n\n\\(C_1\\) is the sill\n\n. . .\n\n\\(d\\) is the distance\n\n. . .\n\n\\(a\\) is the range"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#exponential-function",
    "href": "slides/09_Gaussian_process/index.html#exponential-function",
    "title": "Gaussian process",
    "section": "Exponential function",
    "text": "Exponential function\nNugget : 2 – Sill : 5 – Range : 10"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#matérn-function",
    "href": "slides/09_Gaussian_process/index.html#matérn-function",
    "title": "Gaussian process",
    "section": "Matérn function",
    "text": "Matérn function\n\n\\[C_1\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu}\\frac{d}{a}\\right)^\\nu K_\\nu\\left(\\sqrt{2\\nu}\\frac{d}{a}\\right)\\]\n\n. . .\n\nwhere\n\n\\(\\Gamma(\\nu) = (\\nu - 1)!\\)\n\\(K_\\nu\\) is a modified Bessel function\n\\(d\\) is the distance\n\\(a\\) is the range\n\\(\\nu\\) is a strictly positive parameter giving flexibility to the left tail of the function\n\nif \\(\\nu = 0.5\\) the function converges to an exponential function \\(\\left(C_1\\left(1 - e^{\\frac{-d}{a}}\\right)\\right)\\)\nif \\(\\nu \\rightarrow \\infty\\) the function converges to an Gaussian function (also known as the exponentiated quadratic function) \\(\\left(C_1\\left(1 - e^{\\frac{-d^2}{2a^2}}\\right)\\right)\\)"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#matérn-function-1",
    "href": "slides/09_Gaussian_process/index.html#matérn-function-1",
    "title": "Gaussian process",
    "section": "Matérn function",
    "text": "Matérn function\nSill : 5 – Range : 10 – \\(\\nu\\) : 0.5"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#matérn-function-2",
    "href": "slides/09_Gaussian_process/index.html#matérn-function-2",
    "title": "Gaussian process",
    "section": "Matérn function",
    "text": "Matérn function\nSill : 5 – Range : 10 – \\(\\nu\\) : 5"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-illustrative-example",
    "href": "slides/09_Gaussian_process/index.html#an-illustrative-example",
    "title": "Gaussian process",
    "section": "An illustrative example",
    "text": "An illustrative example\n. . .\nTo present how Gaussian processes can be used, let’s study the distribution of Sylvilagus oviparus in Montréal.\n. . .\nA few characteristics of Sylvilagus oviparus\n\nThey are found mainly in urban parks of Montréal and are very efficient at hiding in hollow trees and burrows.\n\n. . .\n\nThey move well in an urban setting and are not affected by the level of urbanisation\n\n. . .\n\nThey lay their eggs (often pastel-coloured) on the Sunday following the first full moon after the spring equinox."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-typical-member-of-the-species",
    "href": "slides/09_Gaussian_process/index.html#a-typical-member-of-the-species",
    "title": "Gaussian process",
    "section": "A typical member of the species",
    "text": "A typical member of the species"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#distribution-of-s.-oviparus-in-montréal",
    "href": "slides/09_Gaussian_process/index.html#distribution-of-s.-oviparus-in-montréal",
    "title": "Gaussian process",
    "section": "Distribution of S. oviparus in Montréal",
    "text": "Distribution of S. oviparus in Montréal\n. . .\nIn 2015, a survey was carried out to find Sylvilagus oviparus in Montréal’s park. Here are the data . . .\n\n\n\n\n\n\n\n\n\n\nWithin the censused park\n\nBlue parks : observed\nPink parks : not observed"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#model",
    "href": "slides/09_Gaussian_process/index.html#model",
    "title": "Gaussian process",
    "section": "Model",
    "text": "Model\n. . .\nSince we have presence-absence data\n\\[P(y = 1) = \\frac{\\exp(b)}{1 - \\exp(b)}\\]\n. . .\nwhere \\[b \\sim{\\cal N}\\left(0, C_0 +C_1\\left(1 - e^{\\frac{-d}{a}}\\right)\\right)\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fitting-the-model",
    "href": "slides/09_Gaussian_process/index.html#fitting-the-model",
    "title": "Gaussian process",
    "section": "Fitting the model",
    "text": "Fitting the model\nIf we estimate the parameters of the model presented in the previous slide we get a Gaussian process that looks like\n\n\n\n\n\n\n\n\n\n. . .\nWhat can we learn from this model ?"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#prediction-map",
    "href": "slides/09_Gaussian_process/index.html#prediction-map",
    "title": "Gaussian process",
    "section": "Prediction map",
    "text": "Prediction map"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#another-cova",
    "href": "slides/09_Gaussian_process/index.html#another-cova",
    "title": "Gaussian process",
    "section": "Another cova",
    "text": "Another cova"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#kringing",
    "href": "slides/09_Gaussian_process/index.html#kringing",
    "title": "Gaussian process",
    "section": "Kringing",
    "text": "Kringing\nIf we want to interpolate across the region of interest, this is known as kriging.\n. . .\nSimple kriging\nSo far, we have seen the most simplistic Gaussian process where there is not even any intercept that is estimated. In short, the model is constructed using only the covariance function.\n. . .\nIn a linear regression perspective, this means that\n\\[\\mathbf{y}\\sim\\mathcal{N}(0, f(d))\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#kringing-1",
    "href": "slides/09_Gaussian_process/index.html#kringing-1",
    "title": "Gaussian process",
    "section": "Kringing",
    "text": "Kringing\n\nOrdinary kriging\n\nIf we want to account for an intercept in the model such that\n\\[\\mathbf{y}\\sim\\mathcal{N}(\\beta_0, f(d))\\]\nThis is known as ordinary kriging.\n. . .\n\nUniversal kriging\n\nIf we want to account for one or more explanatory variables in the model such that\n\\[\\mathbf{y}\\sim\\mathcal{N}(\\beta_0 + \\mathbf{X}\\beta, f(d))\\] This is known as Universal kriging."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html",
    "href": "slides/08_Complex_hierarchical_model/index.html",
    "title": "Complex hierarchical models",
    "section": "",
    "text": "Mathematically, the basic structure of a hierarchical model is\n\n\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}\\]\n\n\nwhere\n\n\n\n\\(\\mathbf{y}\\) : Vector of response variable\n\\(\\mathbf{X}\\) : Matrix of explanatory variables on which no hierarchies are accounted for\n\\(\\mathbf{Z}\\) : Matrix of explanatory variables on which hierarchies are accounted for\n\\(\\boldsymbol{\\beta}\\) : parameter estimated without a hierarchy\n\\(\\mathbf{b}\\) : parameter estimated with a hierarchy\n\\(\\boldsymbol{\\varepsilon}\\) : a vector that follows a Gaussian distribution such that \\({\\cal N}(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#hierarchical-model-structure-so-far",
    "href": "slides/08_Complex_hierarchical_model/index.html#hierarchical-model-structure-so-far",
    "title": "Complex hierarchical models",
    "section": "",
    "text": "Mathematically, the basic structure of a hierarchical model is\n\n\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}\\]\n\n\nwhere\n\n\n\n\\(\\mathbf{y}\\) : Vector of response variable\n\\(\\mathbf{X}\\) : Matrix of explanatory variables on which no hierarchies are accounted for\n\\(\\mathbf{Z}\\) : Matrix of explanatory variables on which hierarchies are accounted for\n\\(\\boldsymbol{\\beta}\\) : parameter estimated without a hierarchy\n\\(\\mathbf{b}\\) : parameter estimated with a hierarchy\n\\(\\boldsymbol{\\varepsilon}\\) : a vector that follows a Gaussian distribution such that \\({\\cal N}(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#complex-hierarchical-model",
    "href": "slides/08_Complex_hierarchical_model/index.html#complex-hierarchical-model",
    "title": "Complex hierarchical models",
    "section": "“Complex” hierarchical model",
    "text": "“Complex” hierarchical model\nBy “complex” we refer to hierarchical models for which more than one parameters are accounted for in a parameter hierarchy.\n. . .\nAs we will see, there are a number of ways this can complexify the structure of a model in ways that are not always obvious."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies",
    "href": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies",
    "title": "Complex hierarchical models",
    "section": "Interacting hierarchies",
    "text": "Interacting hierarchies\n\nlme4 notation : y ~ (1 | f:g)\n\n. . .\n\nThis model assumes that factors f and g interact to make a hierarchy.\n\n. . .\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f\\times g },\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]\\times g[i]} + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n. . .\n\nwhere \\[\\mathbf{b}_{f\\times g} \\sim \\mathcal{N}\\left(0, \\sigma^2_{f\\times g}\\right)\\]\n\n. . .\n\nNote that a multi-factor hierarchy can be constructed by multiplying the levels of each factor to account for a more complexe hierarchy."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies-1",
    "title": "Complex hierarchical models",
    "section": "Interacting hierarchies",
    "text": "Interacting hierarchies"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Multiple independent hierarchy",
    "text": "Multiple independent hierarchy\n\nlme4 notation : y ~ (1 | f) + (1 | g) or y ~ 1 + (1 | f) + (1 | g)\n\n. . .\n\nThis model assumes there is a hierarchy that varies among two factors that are independent from one another.\n\n. . .\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f} + \\mathbf{b}_{g},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]} + b_{g[i]} + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n. . .\n\nwhere\n\\[\\mathbf{b} \\sim \\mathcal{MNV}\\left(\\begin{bmatrix}\n                                    0\\\\\n                                    0\\\\\n                                  \\end{bmatrix}\n                                ,\n                                \\begin{bmatrix}\n                                  \\sigma^2_f & 0\\\\\n                                  0& \\sigma^2_g\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n. . .\n\nHere, we are dealing with a model that has two intercepts, which are sampled independently so that the \\(b\\)s will change for a sample \\(i\\) only when the the level of factor \\(f\\) and the level of factor \\(g\\) changes independently."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Multiple independent hierarchy",
    "text": "Multiple independent hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies",
    "href": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies",
    "title": "Complex hierarchical models",
    "section": "Nested hierarchies ?",
    "text": "Nested hierarchies ?\n\nlme4 notation : y ~ (1 | f/g) or y ~ (1 | f) + (1 | f:g)\n\n. . .\n\nThis model assumes there is a hierarchy that varies among the levels of factor f and among the levels of factor g but only within the levels of factor f.\n\n. . .\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f}+\\mathbf{b}_{f\\times g},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]}+b_{f[i]\\times g[i]} + \\varepsilon \\quad i = 1\\dots n\\]\n\n. . .\n\nwhere\n\\[\\mathbf{b} \\sim \\mathcal{MNV}\\left(\\begin{bmatrix}\n                                    0\\\\\n                                    0\\\\\n                                  \\end{bmatrix},\n                                \\begin{bmatrix}\n                                  \\sigma^2_f & 0\\\\\n                                  0& \\sigma^2_{f\\times g}\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n. . .\n\nHere, the model has two independent hierarchy, one changes for a sample \\(i\\) as a single intercept hierarchy and the other will change for a sample \\(i\\) only when the level of factor \\(g\\) is within the level of factor \\(f\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies-1",
    "title": "Complex hierarchical models",
    "section": "Nested hierarchies ?",
    "text": "Nested hierarchies ?"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Uncorrelated intercept and slope hierarchy",
    "text": "Uncorrelated intercept and slope hierarchy\n\nlme4 notation : y ~ x + (x || f) or y ~ 1 + x + (1 | f) + (0 + x | f)\n\n. . .\n\nThis model assumes there is a hierarchy that varies independently among the levels of factor f for the intercept and the slope.\n\n. . .\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol\\beta_0+\\boldsymbol\\beta_1\\mathbf{x}+\\mathbf{b}_{0f}+\\mathbf{b}_{1f}\\mathbf{z},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = \\beta_0 + \\beta_1x_i+b_{0f[i]}+b_{1f[i]}z_i + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n. . .\n\nThis is because in this model\n\\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix} \\sim \\mathcal{MNV}\\left(\\begin{bmatrix}\n                                    0\\\\\n                                    0\\\\\n                                  \\end{bmatrix},\n                                \\begin{bmatrix}\n                                  \\sigma^2_{b_0f} & 0\\\\\n                                  0& \\sigma^2_{b_1f}\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n\nNote : In this formulation \\(\\mathbf{x}=\\mathbf{z}\\). Similarly, \\(x_i=z_i\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Uncorrelated intercept and slope hierarchy",
    "text": "Uncorrelated intercept and slope hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#a-small-step-back",
    "href": "slides/08_Complex_hierarchical_model/index.html#a-small-step-back",
    "title": "Complex hierarchical models",
    "section": "A small step back",
    "text": "A small step back\n. . .\n\nThe structure of the hierarchy discussed so far were “mildly complex” in the sense that\n\n\nEven if we estimate parameters of a covariance matrix, they are uncorrelated.\n\\[\n\\begin{bmatrix}\n  \\sigma^2_f & 0\\\\\n  0& \\sigma^2_{g}\\\\\n\\end{bmatrix}\n\\]\n\n. . .\n\nIn Bayesian, having uncorrelated variances allows us to sample variance parameters independently even with multiple factors, which is computationally more efficient.\n\n. . .\n\nFrom this point on, we will look at even more complex covariance structures where the hierarchical levels are not independent from each other."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy\n. . .\n\nlme4 notation : y ~ x + (x | g) or y ~ 1 + x + (1 + x | g)\n\n. . .\n\nThis model assumes the hierarchy between the intercept and the slope is correlated.\n\n. . .\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol\\beta_0 + \\boldsymbol\\beta_1\\mathbf{x}+\\mathbf{b}_{0f} +\\mathbf{b}_{1f}\\mathbf{z},\\mathbf{\\Sigma})\\] or\n\\[y_i = \\beta_0 + \\beta_1x+b_{0f[i]} + b_{1f[i]}z_i + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n. . .\n\nIn this model\n\\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix}\n\\sim \\mathcal{MNV}\\left(\\begin{bmatrix}\n                                    0\\\\\n                                    0\\\\\n                                  \\end{bmatrix},\n                      \\begin{bmatrix}\n                        \\sigma^2_{b_0f} & \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} \\\\\n                        \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} & \\sigma^2_{b_1f}\n                      \\end{bmatrix}\n                \\right)\\]\n\n\nNote : In this formulation \\(\\mathbf{x}=\\mathbf{z}\\). Similarly, \\(x_i=z_i\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy\n. . .\n\nThe covariance structure \\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix}\n\\sim \\mathcal{MNV}\\left(\\begin{bmatrix}\n                                    0\\\\\n                                    0\\\\\n                                  \\end{bmatrix},\n                      \\begin{bmatrix}\n                        \\sigma^2_{b_0f} & \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} \\\\\n                        \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} & \\sigma^2_{b_1f}\n                      \\end{bmatrix}\n                \\right)\\]\nneeds to be discussed a bit more.\n\n. . .\n\nNotation\n\n. . .\n\nIn the covariance matrix,\n\n. . .\n\n\n\\(\\rho_{b_0,b_1}\\) is the correlation between \\(b_0\\) and \\(b_1\\)\n\n\n. . .\n\n\n\\(\\rho_{b_0,b_1}\\sigma_{b_0}\\sigma_{b_1}\\) is the covariance between \\(b_0\\) and \\(b_1\\)\n\n\n. . .\n\nInterpretation\n\n. . .\n\nA covariance matrix with non-zero covariance describes dependence between the \\(b\\)s, which can tell us both the strength of the relation between pairs of parameters and the variance structure."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-2",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-2",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#so-far-what-we-have-done",
    "href": "slides/08_Complex_hierarchical_model/index.html#so-far-what-we-have-done",
    "title": "Complex hierarchical models",
    "section": "So far what we have done",
    "text": "So far what we have done\n. . ."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#what-we-can-do-now",
    "href": "slides/08_Complex_hierarchical_model/index.html#what-we-can-do-now",
    "title": "Complex hierarchical models",
    "section": "What we can do now !",
    "text": "What we can do now !\n. . ."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Combining different types of hierarchy",
    "text": "Combining different types of hierarchy\nWith what we learned so far, it is possible to build more complex model by combining the building blocks we went over in the last few hours.\n. . .\nThe best way to do this is to immerse ourselve into a particular problem.\n. . .\nSo, in the next slides we will discuss about the pumpkinseed (Lepomis gibbosus)"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Combining different types of hierarchy",
    "text": "Combining different types of hierarchy\nFictive context\nPumpkinseed (Lepomis gibbosus) growth has been studied in 15 lakes of Estrie during the famous ichthyology course given at Université de Sherbrooke in the past 30 years.\n. . .\nDuring these ichthyology courses, fish are sampled, measured and tag in all 15 lakes and every year fish are recaptured by students and remeasured. So far, there has been 10 teaching assistants showing there own specific way to measure Pumpkinseed, with, albeit, uneven levels of precision.\n. . .\nYour superviser thinks that Pumpkinseed growth is influenced by water temperature, which has also been sampled every time fish were measured.\n. . .\nAfter 30 years of data gathering, 123 fish have been sampled during 17 consecutive years."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#pumpkinseed-growth-example",
    "href": "slides/08_Complex_hierarchical_model/index.html#pumpkinseed-growth-example",
    "title": "Complex hierarchical models",
    "section": "Pumpkinseed growth example",
    "text": "Pumpkinseed growth example\nQuestion\n. . .\nIs the growth of Pumpkinseed influenced by water temperature?\n\n\n\n\n\n. . .\nTry to build the model that best answer this question\n. . .\nTo have the most precise model, we need to account for lakes area and depth as well as control for the temperature variation in between lakes and for divergences in sampling measurements of each year (wink, wink… teachers assistant may have an influence here as well)."
  }
]