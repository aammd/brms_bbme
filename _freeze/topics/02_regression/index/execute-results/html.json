{
  "hash": "df683f4a95d1b0a8b7fd9b8c04d0f0f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Univariate regression\"\ndescription: |\n  The shortest route to science is a straight line.\nexecute:\n  freeze: true\ncomments:\n  hypothesis: true\nformat:\n  html:\n    code-tools: true\neditor_options: \n  chunk_output_type: console\n---\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))\n```\n:::\n\n\n## Statistical models of Penguin bill morphology.\n\nWe'll be studying the relationship between two numbers about penguin bills. \nSpecifically, we'll ask **\"Are longer bills also deeper?\"**. \nThis question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\n\nLet's begin with plotting the data: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x = bill_len, y = bill_dep)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Bill depth (mm) as predicted by bill length (mm) across the entire `palmerpenguins` dataset.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's write a simple statistical model for these data:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n$$\n\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n:::{.callout-warning}\n# WHERE IS ZERO??\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n:::\n\nIf we fit a model like this **without** thinking about the location of zero, we get some pretty silly answers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(bill_dep ~ bill_len, data = penguins))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    bill_len \n20.88546832 -0.08502128 \n```\n\n\n:::\n:::\n\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\n$$\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n$$\n\nBut, if we take the data as we found it, we're going to be talking about $\\beta_0$ as the depth of a penguin's bill _when the bill has 0 length!_ \nClearly that isn't a very meaningful value. From the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\n\nA very common choice is to **subtract the average** from your independent variable, so that penguins with an average bill length now have an average of 0:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n$$\n\nNow $\\beta_0$ means the average _bill depth_ at the average _bill length_.  It becomes easier to think about priors:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n$$\n\n:::{.callout-note}\n## Exercise\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc.\n:::\n\n## Prior predictive simulations\n\nArmed with this model, it becomes much easier to think about prior predictions.\n\nWe'll make a bunch of lines implied by the equation above. There's two steps:\n\n1. Center the predictor\n2. Make up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_len_centered <- with(penguins,\n                          bill_len - mean(bill_len,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths <- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n```\n:::\n\n\n:::{.callout-warning}\n## Shortcuts to these common tasks\n\nThese tasks are so common that they are automated in helper functions.\n\nFor centering predictors, see the base R function `?scale` (however, doing this by hand is often more convenient)\n\nFor creating a short vector over the range of a predictor, see `modelr::seq_range`. The R package [`modelr`](https://modelr.tidyverse.org/) has many different functions to help with modelling.\n:::\n\nTo simulate, we'll use some matrix algebra, as we saw in lecture:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nknitr::kable(head(X))\n```\n\n::: {.cell-output-display}\n\n\n|   | some_bill_lengths|\n|--:|-----------------:|\n|  1|       -11.8219298|\n|  1|        -8.7663743|\n|  1|        -5.7108187|\n|  1|        -2.6552632|\n|  1|         0.4002924|\n|  1|         3.4558480|\n\n\n:::\n\n```{.r .cell-code}\nknitr::kable(head(B))\n```\n\n::: {.cell-output-display}\n\n\n|       |          |          |           |           |          |           |           |\n|:------|---------:|---------:|----------:|----------:|---------:|----------:|----------:|\n|inters | 17.800961| 19.712418| 19.5981239| 16.9154398| 17.420657| 18.2270240| 19.9969465|\n|slopes |  0.954987| -1.216056|  0.0751899|  0.4172894| -1.196346|  0.2350828| -0.0222059|\n\n\n:::\n\n```{.r .cell-code}\nprior_mus <- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note}\n## Exercise\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n:::\n\n### Simulating Observations\n\nThere are always at least TWO kinds of predictions we can be thinking about: \n\n1. Predicted averages. This is often called a \"confidence\" interval for a regression line.\n2. Predicted observations. This is often called a \"prediction\" interval.\n\nWe can use the full model to simulate observations! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\nsigmas <- rexp(7, rate = 0.3)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nprior_mus <- X %*% B\n\nprior_obs <- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] <- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nTidyverse style for those who indulge:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |> \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |> \n  rowwise() |> \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |> \n  tidyr::unnest(cols = c(\"x\", \"avg\", \"obs\")) |> \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\n* Experiment with priors that are \"too narrow\" or \"too wide\". \n* Try a different distribution than the one used\n* Instead of bill size, imagine that we are applying this model to YOUR data. What would you change?\n:::\n\n## Linear regression in brms\n\nNow we write some brms code for this model. \nWe'll begin with a simple model that has no posterior predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## get data ready\npeng_dep_len_df <- penguins |> \n  tidyr::drop_na(bill_dep, bill_len) |> \n  mutate(bill_len_cen = bill_len - mean(bill_len))\n\n# write formula\nnormal_reg_bf <- bf(bill_dep ~ bill_len_cen, family = gaussian())\n\n## make priors\nget_prior(normal_reg_bf, data = peng_dep_len_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class         coef group resp dpar nlpar lb ub tag\n                  (flat)         b                                             \n                  (flat)         b bill_len_cen                                \n student_t(3, 17.3, 2.5) Intercept                                             \n    student_t(3, 0, 2.5)     sigma                                     0       \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n\n```{.r .cell-code}\nnormal_reg_prior <- c(\n  prior(normal(0, .5), class = \"b\"),\n  prior(normal(17, 2), class = \"Intercept\"),\n  prior(exponential(.5), class = \"sigma\", lb = 0)\n)\n\nnormal_reg_brm <- brm(formula = normal_reg_bf,\n                      prior = normal_reg_prior,\n                      data = peng_dep_len_df, file = here::here(\"topics/02_regression/normal_regression.rds\"),\n                      file_refit = \"on_change\", refresh = 0L)\n```\n:::\n\n\nget the variable names\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidybayes::get_variables(normal_reg_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"    \"b_bill_len_cen\" \"sigma\"          \"Intercept\"     \n [5] \"lprior\"         \"lp__\"           \"accept_stat__\"  \"stepsize__\"    \n [9] \"treedepth__\"    \"n_leapfrog__\"   \"divergent__\"    \"energy__\"      \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_brm |> \n  bayesplot::mcmc_areas(pars = c(\"b_bill_len_cen\", \"Intercept\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\n**Discussion** : Look just at the posterior distribution of the slope right above. \nDo we have evidence that there's a relationship between bill length and bill depth?\n:::\n\n## Posterior predictions in R\n\nWe can calculate a posterior prediction line directly in R for these data.\nI'll show each step in this workflow separately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_brm |> \n  spread_rvars(b_bill_len_cen, Intercept, sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   b_bill_len_cen  Intercept        sigma\n       <rvar[1d]> <rvar[1d]>   <rvar[1d]>\n1  -0.085 ± 0.019   17 ± 0.1  1.9 ± 0.072\n```\n\n\n:::\n:::\n\n\n`tidybayes` helps us extract the posterior distribution of the parameters into a convenient object called an `rvar`. \nLearn more about tidybayes [here](http://mjskay.github.io/tidybayes/articles/tidybayes.html) and about the rvar datatype [here](https://mc-stan.org/posterior/articles/rvar.html)\n\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline <- normal_reg_brm |> \n  tidybayes::spread_rvars(b_bill_len_cen, Intercept) |> \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |> \n  mutate(mu = Intercept + b_bill_len_cen*x)\n\nnormal_reg_predline\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n   b_bill_len_cen  Intercept     x         mu\n       <rvar[1d]> <rvar[1d]> <dbl> <rvar[1d]>\n1  -0.085 ± 0.019   17 ± 0.1 -15    18 ± 0.31\n2  -0.085 ± 0.019   17 ± 0.1  -7.5  18 ± 0.18\n3  -0.085 ± 0.019   17 ± 0.1   0    17 ± 0.10\n4  -0.085 ± 0.019   17 ± 0.1   7.5  17 ± 0.18\n5  -0.085 ± 0.019   17 ± 0.1  15    16 ± 0.31\n```\n\n\n:::\n:::\n\n\nFinally we'll plot these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline |> \n  ggplot(aes(x = x, dist = mu)) + \n  stat_lineribbon() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n#### Using posterior draws individually\n\nThe above workflow makes a nice figure, but perhaps it helps to see the individual lines to understand what is happening here. \nWe can get these with another tidybayes function `spread_draws`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline_draws <- normal_reg_brm |> \n  tidybayes::spread_draws(b_bill_len_cen, Intercept, ndraws = 12)\n\nknitr::kable(normal_reg_predline_draws)\n```\n\n::: {.cell-output-display}\n\n\n| .chain| .iteration| .draw| b_bill_len_cen| Intercept|\n|------:|----------:|-----:|--------------:|---------:|\n|      2|        446|  1446|     -0.0706029|  17.21994|\n|      4|        614|  3614|     -0.0881541|  17.21511|\n|      2|         33|  1033|     -0.0790708|  17.01977|\n|      1|        325|   325|     -0.0812637|  17.22786|\n|      1|        211|   211|     -0.0759466|  17.27884|\n|      1|        551|   551|     -0.1104298|  17.22524|\n|      4|        735|  3735|     -0.0878046|  17.39996|\n|      3|        282|  2282|     -0.0927001|  17.10126|\n|      2|        321|  1321|     -0.0627590|  17.24024|\n|      3|        194|  2194|     -0.0843824|  17.15836|\n|      1|         43|    43|     -0.1089193|  17.16261|\n|      1|        912|   912|     -0.0696936|  17.06779|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline_draws |> \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |> \n  mutate(mu = Intercept + b_bill_len_cen*x) |> \n  ggplot(aes(x = x, y = mu, group = .draw)) + \n  geom_line() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n## Posterior predicted observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(bill_len_cen = seq(from = -15, to = 15, length.out = 5)) |> \n  tidybayes::add_predicted_rvars(normal_reg_brm) |> \n  ggplot(aes(x = bill_len_cen, ydist = .prediction)) + \n  stat_lineribbon() +\n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n### EXERCISE\nExtend this model to include species. Specifically, let each species have its own value of the `intercept`. This involves combining this regression example with the previous activity on discrete predictors.\n\nWhen you're done, look at the resulting summary of coefficients. What do you notice that's different?\n::: \n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\nWe set up a list for this model just as we did before. \nNote that this time we are using TRIPLE the `pred_values`, because we want to run independent predictions for each species.\n\n\n\n:::\n\n<!-- ### Plotting posterior predictions -->\n\n<!-- Using `stat_lineribbon()`, let's plot the average and predicted intervals for this regression. -->\n\n<!-- ```{r} -->\n<!-- #| layout-ncol: 2 -->\n<!-- #| warning: false -->\n<!-- bill_posterior <- normal_reg_spp_post |>  -->\n<!--   tidybayes::spread_rvars(post_bill_dep_average[i], -->\n<!--                           post_bill_dep_obs[i]) |> -->\n<!--   mutate(bill_length = data_list_spp$pred_values[i], -->\n<!--          spp = data_list_spp$pred_spp_id) |>  -->\n<!--   mutate(spp = as.factor(levels(penguins$species)[spp])) -->\n\n<!-- bill_posterior |>  -->\n<!--   ggplot(aes(x = bill_length, -->\n<!--              ydist = post_bill_dep_average, -->\n<!--              fill = spp,  -->\n<!--              colour = spp)) +  -->\n<!--   tidybayes::stat_lineribbon() +  -->\n<!--   geom_point(aes(x = bill_length_center, -->\n<!--                  y = bill_depth_mm, -->\n<!--                  fill = species, colour = species), -->\n<!--              data = penguins_no_NA,  -->\n<!--              inherit.aes = FALSE) +    -->\n<!--   scale_fill_brewer(palette = \"Set2\") + -->\n<!--   scale_color_brewer(palette = \"Dark2\") +  -->\n<!--   labs(title = \"Average response\") -->\n\n<!-- bill_posterior |>  -->\n<!--   ggplot(aes(x = bill_length, -->\n<!--              dist = post_bill_dep_obs, -->\n<!--              fill = spp, -->\n<!--              colour = spp)) +  -->\n<!--   tidybayes::stat_lineribbon() +  -->\n<!--   geom_point(aes(x = bill_length_center, -->\n<!--                  y = bill_depth_mm, -->\n<!--                  colour = species), -->\n<!--              data = penguins_no_NA,  -->\n<!--              inherit.aes = FALSE) +  -->\n<!--   scale_fill_brewer(palette = \"Set2\") + -->\n<!--   scale_color_brewer(palette = \"Dark2\") +  -->\n<!--   labs(title = \"Predicted observations\") +  -->\n<!--   facet_wrap(~spp, ncol = 1) -->\n\n<!-- ``` -->\n\n## Exercise! \n\n1. We have one model without species identity as an independent variable, and one which includes species. Look at the difference in $\\sigma$ between these two models. Why did the value change? \n2. **Posterior predictions** Edit the generated quantities blocks of `normal_regression.stan` and `normal_regression_spp.stan`  to create replicate observations for all N observations (that is, `yrep` as we did in the model of [bill depth previously](../discrete_predictor/index.qmd)). Use this to compare the models using `bayesplot::ppc_dens_overlay()` or another function from the bayesplot package.\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}