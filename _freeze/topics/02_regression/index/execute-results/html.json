{
  "hash": "b995d2e136c3d44e67ae3b04eaea541b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Univariate regression\"\ndescription: |\n  The shortest route to science is a straight line.\nexecute:\n  freeze: true\ncomments:\n  hypothesis: true\nformat:\n  html:\n    code-tools: true\neditor_options: \n  chunk_output_type: console\n---\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))\n```\n:::\n\n\n## Statistical models of Penguin bill morphology.\n\nWe'll be studying the relationship between two numbers about penguin bills. \nSpecifically, we'll ask **\"Are longer bills also deeper?\"**. \nThis question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\n\nLet's begin with plotting the data: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x = bill_len, y = bill_dep)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Bill depth (mm) as predicted by bill length (mm) across the entire `palmerpenguins` dataset.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's write a simple statistical model for these data:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n$$\n\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n:::{.callout-warning}\n# WHERE IS ZERO??\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n:::\n\nIf we fit a model like this **without** thinking about the location of zero, we get some pretty silly answers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(bill_dep ~ bill_len, data = penguins))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    bill_len \n20.88546832 -0.08502128 \n```\n\n\n:::\n:::\n\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\n$$\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n$$\n\nBut, if we take the data as we found it, we're going to be talking about $\\beta_0$ as the depth of a penguin's bill _when the bill has 0 length!_ \nClearly that isn't a very meaningful value. From the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\n\nA very common choice is to **subtract the average** from your independent variable, so that penguins with an average bill length now have an average of 0:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n$$\n\nNow $\\beta_0$ means the average _bill depth_ at the average _bill length_.  It becomes easier to think about priors:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n$$\n\n:::{.callout-note}\n## Exercise\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc.\n:::\n\n## Prior predictive simulations\n\nArmed with this model, it becomes much easier to think about prior predictions.\n\nWe'll make a bunch of lines implied by the equation above. There's two steps:\n\n1. Center the predictor\n2. Make up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_len_centered <- with(penguins,\n                          bill_len - mean(bill_len,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths <- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n```\n:::\n\n\n:::{.callout-warning}\n## Shortcuts to these common tasks\n\nThese tasks are so common that they are automated in helper functions.\n\nFor centering predictors, see the base R function `?scale` (however, doing this by hand is often more convenient)\n\nFor creating a short vector over the range of a predictor, see `modelr::seq_range`. The R package [`modelr`](https://modelr.tidyverse.org/) has many different functions to help with modelling.\n:::\n\nTo simulate, we'll use some matrix algebra, as we saw in lecture:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nknitr::kable(head(X))\n```\n\n::: {.cell-output-display}\n\n\n|   | some_bill_lengths|\n|--:|-----------------:|\n|  1|       -11.8219298|\n|  1|        -8.7663743|\n|  1|        -5.7108187|\n|  1|        -2.6552632|\n|  1|         0.4002924|\n|  1|         3.4558480|\n\n\n:::\n\n```{.r .cell-code}\nknitr::kable(head(B))\n```\n\n::: {.cell-output-display}\n\n\n|       |           |           |         |          |          |          |           |\n|:------|----------:|----------:|--------:|---------:|---------:|---------:|----------:|\n|inters | 18.1597247| 17.4772899| 15.21873| 16.234098| 16.567020| 18.138464| 14.7523552|\n|slopes | -0.4448028| -0.4265729| -0.20687| -0.005596|  0.433877| -0.499978| -0.9988436|\n\n\n:::\n\n```{.r .cell-code}\nprior_mus <- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note}\n## Exercise\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n:::\n\n### Simulating Observations\n\nThere are always at least TWO kinds of predictions we can be thinking about: \n\n1. Predicted averages. This is often called a \"confidence\" interval for a regression line.\n2. Predicted observations. This is often called a \"prediction\" interval.\n\nWe can use the full model to simulate observations! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\nsigmas <- rexp(7, rate = 0.3)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nprior_mus <- X %*% B\n\nprior_obs <- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] <- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nTidyverse style for those who indulge:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |> \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |> \n  rowwise() |> \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |> \n  tidyr::unnest(cols = c(\"x\", \"avg\", \"obs\")) |> \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\n* Experiment with priors that are \"too narrow\" or \"too wide\". \n* Try a different distribution than the one used\n* Instead of bill size, imagine that we are applying this model to YOUR data. What would you change?\n:::\n\n## Linear regression in brms\n\nNow we write some brms code for this model. \nWe'll begin with a simple model that has no posterior predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## get data ready\npeng_dep_len_df <- penguins |> \n  tidyr::drop_na(bill_dep, bill_len) |> \n  mutate(bill_len_cen = bill_len - mean(bill_len))\n\n# write formula\nnormal_reg_bf <- bf(bill_dep ~ 1 + bill_len_cen, family = gaussian())\n\n## make priors\nget_prior(normal_reg_bf, data = peng_dep_len_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class         coef group resp dpar nlpar lb ub tag\n                  (flat)         b                                             \n                  (flat)         b bill_len_cen                                \n student_t(3, 17.3, 2.5) Intercept                                             \n    student_t(3, 0, 2.5)     sigma                                     0       \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n\n```{.r .cell-code}\nnormal_reg_prior <- c(\n  prior(normal(0, .5), class = \"b\"),\n  prior(normal(17, 2), class = \"Intercept\"),\n  prior(exponential(.5), class = \"sigma\", lb = 0)\n)\n\nnormal_reg_brm <- brm(formula = normal_reg_bf,\n                      prior = normal_reg_prior,\n                      data = peng_dep_len_df,\n                      file = here::here(\"topics/02_regression/normal_regression.rds\"),\n                      file_refit = \"on_change\", refresh = 0L)\n```\n:::\n\n\nget the variable names\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidybayes::get_variables(normal_reg_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"    \"b_bill_len_cen\" \"sigma\"          \"Intercept\"     \n [5] \"lprior\"         \"lp__\"           \"accept_stat__\"  \"stepsize__\"    \n [9] \"treedepth__\"    \"n_leapfrog__\"   \"divergent__\"    \"energy__\"      \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_brm |> \n  bayesplot::mcmc_areas(pars = c(\"b_bill_len_cen\", \"Intercept\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnormal_reg_brm |> \n  bayesplot::mcmc_areas(pars = \"b_bill_len_cen\") + \n  coord_cartesian(xlim = c(-0.16, 0.16))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\n**Discussion** : Look just at the posterior distribution of the slope right above. \nDo we have evidence that there's a relationship between bill length and bill depth?\n:::\n\n## Posterior predictions in R\n\nWe can calculate a posterior prediction line directly in R for these data.\nI'll show each step in this workflow separately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_brm |> \n  tidybayes::spread_rvars(b_bill_len_cen, Intercept, sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   b_bill_len_cen  Intercept        sigma\n       <rvar[1d]> <rvar[1d]>   <rvar[1d]>\n1  -0.085 ± 0.019   17 ± 0.1  1.9 ± 0.072\n```\n\n\n:::\n:::\n\n\n`tidybayes` helps us extract the posterior distribution of the parameters into a convenient object called an `rvar`. \nLearn more about tidybayes [here](http://mjskay.github.io/tidybayes/articles/tidybayes.html) and about the rvar datatype [here](https://mc-stan.org/posterior/articles/rvar.html)\n\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline <- normal_reg_brm |> \n  tidybayes::spread_rvars(b_bill_len_cen, Intercept) |> \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |> \n  mutate(mu = Intercept + b_bill_len_cen*x)\n\nnormal_reg_predline\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n   b_bill_len_cen  Intercept     x         mu\n       <rvar[1d]> <rvar[1d]> <dbl> <rvar[1d]>\n1  -0.085 ± 0.019   17 ± 0.1 -15    18 ± 0.31\n2  -0.085 ± 0.019   17 ± 0.1  -7.5  18 ± 0.18\n3  -0.085 ± 0.019   17 ± 0.1   0    17 ± 0.10\n4  -0.085 ± 0.019   17 ± 0.1   7.5  17 ± 0.18\n5  -0.085 ± 0.019   17 ± 0.1  15    16 ± 0.31\n```\n\n\n:::\n:::\n\n\nFinally we'll plot these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline |> \n  ggplot(aes(x = x, dist = mu)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n#### Using posterior draws individually\n\nThe above workflow makes a nice figure, but perhaps it helps to see the individual lines to understand what is happening here. \nWe can get these with another tidybayes function `spread_draws`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline_draws <- normal_reg_brm |> \n  tidybayes::spread_draws(b_bill_len_cen, Intercept, ndraws = 12)\n\nknitr::kable(normal_reg_predline_draws)\n```\n\n::: {.cell-output-display}\n\n\n| .chain| .iteration| .draw| b_bill_len_cen| Intercept|\n|------:|----------:|-----:|--------------:|---------:|\n|      4|        482|  3482|     -0.0882432|  17.21116|\n|      2|        508|  1508|     -0.0885217|  17.29615|\n|      2|        495|  1495|     -0.1132187|  17.05900|\n|      1|        995|   995|     -0.0665854|  17.21139|\n|      4|        486|  3486|     -0.0996216|  17.11091|\n|      3|        368|  2368|     -0.0819782|  17.15651|\n|      2|        560|  1560|     -0.1071510|  17.12442|\n|      1|        640|   640|     -0.0819936|  17.26048|\n|      4|        252|  3252|     -0.1015765|  16.97635|\n|      4|        282|  3282|     -0.1161147|  17.14538|\n|      4|        166|  3166|     -0.1056779|  17.27641|\n|      2|        317|  1317|     -0.0963531|  17.14618|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline_draws |> \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |> \n  mutate(mu = Intercept + b_bill_len_cen*x) |> \n  ggplot(aes(x = x, y = mu, group = .draw)) + \n  geom_line() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n## Posterior predicted observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(bill_len_cen = seq(from = -15, to = 15, length.out = 5)) |> \n  tidybayes::add_predicted_rvars(normal_reg_brm) |> \n  ggplot(aes(x = bill_len_cen, ydist = .prediction)) + \n  stat_lineribbon() +\n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(normal_reg_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: bill_dep ~ bill_len_cen \n   Data: peng_dep_len_df (Number of observations: 342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       17.15      0.10    16.96    17.36 1.00     3883     2619\nbill_len_cen    -0.09      0.02    -0.12    -0.05 1.00     3805     2949\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.93      0.07     1.79     2.07 1.00     4058     2799\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n### EXERCISE\nExtend this model to include species. Specifically, let each species have its own value of the `intercept`. This involves combining this regression example with the previous activity on discrete predictors.\n\nWhen you're done, look at the resulting summary of coefficients. What do you notice that's different?\n::: \n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## dataset\nglimpse(peng_dep_len_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 342\nColumns: 9\n$ species      <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A…\n$ island       <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, To…\n$ bill_len     <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.…\n$ bill_dep     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.…\n$ flipper_len  <int> 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 180, 18…\n$ body_mass    <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 330…\n$ sex          <fct> male, female, female, female, male, female, male, NA, NA,…\n$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…\n$ bill_len_cen <dbl> -4.82193, -4.42193, -3.62193, -7.22193, -4.62193, -5.0219…\n```\n\n\n:::\n\n```{.r .cell-code}\n## formula\nbill_dep_len_sp_bf <- bf(bill_dep ~ 0 + bill_len_cen + species, family = gaussian())\n\nget_prior(bill_dep_len_sp_bf, data = peng_dep_len_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                prior class             coef group resp dpar nlpar lb ub tag\n               (flat)     b                                                 \n               (flat)     b     bill_len_cen                                \n               (flat)     b    speciesAdelie                                \n               (flat)     b speciesChinstrap                                \n               (flat)     b    speciesGentoo                                \n student_t(3, 0, 2.5) sigma                                         0       \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n```\n\n\n:::\n\n```{.r .cell-code}\nbill_dep_len_sp_prior <- c(\n  prior(normal(17, 2), class = \"b\", coef = \"speciesAdelie\"),\n  prior(normal(17, 2), class = \"b\", coef = \"speciesChinstrap\"),\n  prior(normal(17, 2), class = \"b\", coef = \"speciesGentoo\"),\n  prior(normal(0, .5), class = \"b\", coef = \"bill_len_cen\"),\n  prior(exponential(.5), class = \"sigma\", lb = 0)\n)\n\nbill_dep_len_sp_brm <- brm(formula = bill_dep_len_sp_bf, \n                           data = peng_dep_len_df,\n                           prior = bill_dep_len_sp_prior, \n                           file = here::here(\"topics/02_regression/bill_dep_len_sp_brm.rds\"),\n                           file_refit = \"on_change\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms::stancode(bill_dep_len_sp_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  vector[N] Y;  // response variable\n  int<lower=1> K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real<lower=0> sigma;  // dispersion parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += normal_lpdf(b[1] | 0, 0.5);\n  lprior += normal_lpdf(b[2] | 17, 2);\n  lprior += normal_lpdf(b[3] | 17, 2);\n  lprior += normal_lpdf(b[4] | 17, 2);\n  lprior += exponential_lpdf(sigma | 0.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += normal_id_glm_lpdf(Y | X, 0, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyr::expand_grid(bill_len_cen = seq(from = -15, to = 15, length.out = 5),\n                   species = unique(peng_dep_len_df$species)) |>\n  tidybayes::add_epred_rvars(bill_dep_len_sp_brm) |>\n  ggplot(aes(x = bill_len_cen, dist = .epred, group = species)) + \n  stat_dist_lineribbon() + \n  facet_wrap(~species) + \n  geom_point(aes(x = bill_len_cen, y = bill_dep), \n             data = peng_dep_len_df, inherit.aes = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(bill_dep_len_sp_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: bill_dep ~ 0 + bill_len_cen + species \n   Data: peng_dep_len_df (Number of observations: 342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nbill_len_cen         0.20      0.02     0.16     0.23 1.00     1348     1740\nspeciesAdelie       19.36      0.12    19.12    19.59 1.00     1512     2094\nspeciesChinstrap    17.45      0.15    17.17    17.74 1.00     1855     1777\nspeciesGentoo       14.28      0.11    14.07    14.49 1.00     1879     2066\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.96      0.04     0.89     1.03 1.00     3612     2872\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<!-- ### Plotting posterior predictions -->\n\n<!-- Using `stat_lineribbon()`, let's plot the average and predicted intervals for this regression. -->\n\n<!-- ```{r} -->\n<!-- #| layout-ncol: 2 -->\n<!-- #| warning: false -->\n<!-- bill_posterior <- normal_reg_spp_post |>  -->\n<!--   tidybayes::spread_rvars(post_bill_dep_average[i], -->\n<!--                           post_bill_dep_obs[i]) |> -->\n<!--   mutate(bill_length = data_list_spp$pred_values[i], -->\n<!--          spp = data_list_spp$pred_spp_id) |>  -->\n<!--   mutate(spp = as.factor(levels(penguins$species)[spp])) -->\n\n<!-- bill_posterior |>  -->\n<!--   ggplot(aes(x = bill_length, -->\n<!--              ydist = post_bill_dep_average, -->\n<!--              fill = spp,  -->\n<!--              colour = spp)) +  -->\n<!--   tidybayes::stat_lineribbon() +  -->\n<!--   geom_point(aes(x = bill_length_center, -->\n<!--                  y = bill_depth_mm, -->\n<!--                  fill = species, colour = species), -->\n<!--              data = penguins_no_NA,  -->\n<!--              inherit.aes = FALSE) +    -->\n<!--   scale_fill_brewer(palette = \"Set2\") + -->\n<!--   scale_color_brewer(palette = \"Dark2\") +  -->\n<!--   labs(title = \"Average response\") -->\n\n<!-- bill_posterior |>  -->\n<!--   ggplot(aes(x = bill_length, -->\n<!--              dist = post_bill_dep_obs, -->\n<!--              fill = spp, -->\n<!--              colour = spp)) +  -->\n<!--   tidybayes::stat_lineribbon() +  -->\n<!--   geom_point(aes(x = bill_length_center, -->\n<!--                  y = bill_depth_mm, -->\n<!--                  colour = species), -->\n<!--              data = penguins_no_NA,  -->\n<!--              inherit.aes = FALSE) +  -->\n<!--   scale_fill_brewer(palette = \"Set2\") + -->\n<!--   scale_color_brewer(palette = \"Dark2\") +  -->\n<!--   labs(title = \"Predicted observations\") +  -->\n<!--   facet_wrap(~spp, ncol = 1) -->\n\n<!-- ``` -->\n\n## Exercise! \n\n1. We have one model without species identity as an independent variable, and one which includes species. Look at the difference in $\\sigma$ between these two models. Why did the value change? \n2. **Posterior predictions** Compare the model with species identity to the one without it, by performing posterior predictive checks for each of them (e.g. using ``pp_check(..., type = \"dens_overlay\"))` ) which model do you prefer?\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}