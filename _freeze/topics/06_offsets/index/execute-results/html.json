{
  "hash": "e59111a7b92a9aa1a8db176fcb988ac5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Controlling for effort with `offset()`\"\neditor_options: \n  chunk_output_type: console\n---\n\n\nWe're going to search a plot and try to count all the individuals of a rare orchid. These orchids are rather rare, and even where they are common you're not likely to find them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'brms' package (version 2.23.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'brms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'tidybayes'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n```\n\n\n:::\n\n```{.r .cell-code}\n# how many orchids in a hectare? that is 100 x 100 m, or 10 000 square meters\nplants_per_sq_m <- 4/1e4\ncount_per_ha <- rpois(1000, lambda = plants_per_sq_m*1e4)\nhist(count_per_ha)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(count_per_ha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.949\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(count_per_ha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.968367\n```\n\n\n:::\n:::\n\n\nLet's say that surveys originally followed a protocol of searching rectangular transects for this plant. A single transect is 5m x 100m. Later, survey protocol was changed and the length of the transect was increased to 250m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlabels <- c(\"small\", \"large\")\nsizes <- c(5*100, 5*250)\nflower_survey <- data_frame(group_id = rep(c(1, 2), each = 35),\n                            area_surveyed = sizes[group_id]) |> \n  mutate(flowers = rpois(length(group_id), lambda = area_surveyed*plants_per_sq_m))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(flower_survey, aes(x = flowers)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWe want to estimate the true density of the plant, which we know is 4\\times 10^{-4}. \n\nWhat happens if we ignore the area surveyed?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(glm(flowers ~ 1, family = \"poisson\", data = flower_survey)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.4714286 \n```\n\n\n:::\n:::\n\n\nnot surprisingly a VERY wrong answer! \n\nCan we \"standardize\" by count?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(with(flower_survey, flowers/area_surveyed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0005828571\n```\n\n\n:::\n\n```{.r .cell-code}\n# or:\n\nlm( I(flowers/area_surveyed) ~ 1, data = flower_survey)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = I(flowers/area_surveyed) ~ 1, data = flower_survey)\n\nCoefficients:\n(Intercept)  \n  0.0005829  \n```\n\n\n:::\n:::\n\n\nThat gets us pretty close to the average count! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflower_survey_stand <- mutate(flower_survey, per_m2 = flowers/area_surveyed)\n\nflowers_gauss_brm <- brm(formula = bf(per_m2 ~ 1, family = \"gaussian\"),\n    prior = c(\n      prior(normal(0.0005, 0.0001), class = \"Intercept\"),\n      prior(exponential(1), class = \"sigma\")\n    ),\n    data = flower_survey_stand,\n    refresh = 0L,\n    file = here::here(\"topics/06_offsets/flowers_gauss_brm\")\n    )\n```\n:::\n\n\nIt's really not far from the truth! But the model makes awkward predictions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(flowers_gauss_brm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_areas(flowers_gauss_brm, pars = \"Intercept\") + \n  geom_vline(xintercept = plants_per_sq_m, col = \"red\", size = 3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Using a Poisson offset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflower_pois_bf <- bf(flowers ~ 1 + offset(log(area_surveyed)), family = \"poisson\")\n\nget_prior(flower_pois_bf, data = flower_survey)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept ~ student_t(3, -8.97275346435927, 2.5)\n```\n\n\n:::\n:::\n\n\nNote that there is only one parameter, because the offset does not add  parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflower_pois_prior <- prior(normal(log(0.0005), .5), class = \"Intercept\")\n\nflower_pois_brm <- brm(flower_pois_bf, \n                       data = flower_survey,\n                       prior= flower_pois_prior, \n                       refresh = 0L, \n                       file = here::here(\"topics/06_offsets/flower_pois_brm.rds\"))\n```\n:::\n\n\nPosterior predictive check shows a more satisfying prediction of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(flower_pois_brm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n  \nWe get a similar posterior:  \n  \n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nget_variables(flower_pois_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"   \"Intercept\"     \"lprior\"        \"lp__\"         \n [5] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n [9] \"divergent__\"   \"energy__\"     \n```\n\n\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_areas(flower_pois_brm, pars = \"Intercept\") + \n  geom_vline(xintercept = log(plants_per_sq_m), col = \"red\", size = 3)\n```\n\n::: {.cell-output-display}\n![Two ways to plot the posterior distribution of average flower number and compare it to the true values.](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nflower_pois_brm |> \n  tidybayes::gather_draws(Intercept) |> \n  mutate(average = exp(.value)) |> \n  ggplot(aes(x = average)) + \n  tidybayes::stat_halfeye() + \n  geom_vline(xintercept = plants_per_sq_m, col = \"red\", size = 3)\n```\n\n::: {.cell-output-display}\n![Two ways to plot the posterior distribution of average flower number and compare it to the true values.](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n## Why does it work\n\nThe `offset()` function simply adds a term to our linear predictor and gives it a coefficient of exactly 1 -- that is, it does not multiply it by anything at all. \nTogether with the link function, it lets us scale the mean of the model by a constant term. This helps us to control for exposure or effort, when you find more of the response because you search over a larger space:\n\n$$\n\\begin{align}\ny &\\sim \\text{Pois}(\\lambda) \\\\\ny &\\sim \\text{Pois}(e^\\alpha) \\\\\ny &\\sim \\text{Pois}(e^{\\beta_0 + \\text{offset}(\\ln(\\text{effort}))}) \\\\\ny &\\sim \\text{Pois}(e^{\\beta_0 + \\ln(\\text{effort})}) \\\\\ny &\\sim \\text{Pois}(e^{\\beta_0}e^{\\ln(\\text{effort})}) \\\\\ny &\\sim \\text{Pois}(\\text{effort}\\times e^{\\beta_0}) \\\\\n\\end{align}\n$$\n\n\n##  Binary data\n\nDetection of frog calls in a survey. Listening for the same time (but that doesn't always happen!)\n\nsurvey durations in minutes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrog_data <- data_frame(survey_times = rep(c(30, 10, 15, 45), each = 12))\n\n## calls on average once every 13 minutes\nfrog_calling_rate_per_min <- 1/13\n\nfrog_calls <- frog_data |> \n  rowwise() |> \n  mutate(calls_per_min = list(\n    rbinom(n = survey_times,\n           size = 1,\n           prob = frog_calling_rate_per_min\n           )\n    ))\n\nfrog_calls$calls_per_min[21]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] 0 0 0 0 0 1 0 0 0 0\n```\n\n\n:::\n\n```{.r .cell-code}\nfrog_call_pa <- frog_calls |> \n  mutate(call_pa = as.numeric(sum(calls_per_min)>0))\n```\n:::\n\n\nLink to the `rowwise()` vignette\n\nmodel with a glm in base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncall_bf <- bf(call_pa ~ 1, family = bernoulli(link = \"logit\"))\n\nget_prior(call_bf, data = frog_call_pa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept ~ student_t(3, 0, 2.5)\n```\n\n\n:::\n\n```{.r .cell-code}\n## true parameter\ncurve(plogis(x), xlim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncurve(qlogis(x), xlim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nqlogis(frog_calling_rate_per_min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.484907\n```\n\n\n:::\n\n```{.r .cell-code}\ncall_prior <- c(\n  prior(normal(-2.5, .2), class = \"Intercept\")\n)\n\ncall_brm <- brm(call_bf, \n    family = bernoulli(), \n    prior = call_prior,\n    file = here::here(\"topics/06_offsets/call_brm.rds\"),\n    data = frog_call_pa)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/Rcpp/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/unsupported\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/BH/include\" -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/src/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppParallel/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-xupQTd/r-base-4.5.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/Core:19,\n                 from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/Dense:1,\n                 from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from <command-line>:\n/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include <cmath>\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:202: foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.008 seconds (Sampling)\nChain 1:                0.017 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 2:                0.008 seconds (Sampling)\nChain 2:                0.017 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.018 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.009 seconds (Sampling)\nChain 4:                0.019 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(call_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: call_pa ~ 1 \n   Data: frog_call_pa (Number of observations: 48) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.37      0.18    -1.71    -1.03 1.00     1384     2207\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nplogis(-1.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1978161\n```\n\n\n:::\n:::\n\n\nThe right way to control for unequal effort in a logistic regression (AKA a bernoulli GLM) is via the \"cumulative log-log link\" or `cloglog` link.\n\nThe equation for this function is\n\n$$\n1 - e^{-e^a}\n$$\n\nWe can plot this equation against the logistic curve like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(1 - exp(-exp(x)), xlim = c(-4, 4), lwd = 2)\ncurve(plogis(x), col = \"blue\", add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nwhere $a$ is the linear predictor (ie our model equation).\n\nNotice what happens when we add a log offset to this equation:\n\n$$\n\\begin{align}\np &= 1 - e^{-e^a} \\\\\n&= 1 - e^{-e^{\\beta_0 + \\text{offset}(\\ln(\\text{effort}))}} \\\\\n&= 1 - e^{-e^{\\beta_0} \\times \\text{effort}} \\\\\n&= 1 - \\left( e^{-e^{\\beta_0}} \\right)^\\text{effort}\n\\end{align}\n\n\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncall_cll_bf <- bf(call_pa ~ 1 + offset(log(survey_times)),\n                  family = bernoulli(link = \"cloglog\"))\n\nget_prior(call_cll_bf, data = frog_call_pa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept ~ student_t(3, 0, 2.5)\n```\n\n\n:::\n\n```{.r .cell-code}\nqlogis(frog_calling_rate_per_min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.484907\n```\n\n\n:::\n\n```{.r .cell-code}\ncall_prior <- c(\n  prior(normal(-2.5, .2), class = \"Intercept\")\n)\n\ncall_cll_brm <- brm(formula = call_cll_bf,\n    prior = call_prior,\n    refresh = 0L,\n    data = frog_call_pa, file = \"topics/06_offsets/call_cll_brm.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/Rcpp/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/unsupported\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/BH/include\" -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/src/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppParallel/include/\"  -I\"/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-xupQTd/r-base-4.5.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/Core:19,\n                 from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/Dense:1,\n                 from /home/andrew/R/x86_64-pc-linux-gnu-library/4.5/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from <command-line>:\n/home/andrew/R/x86_64-pc-linux-gnu-library/4.5/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include <cmath>\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:202: foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(call_cll_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = cloglog \nFormula: call_pa ~ 1 + offset(log(survey_times)) \n   Data: frog_call_pa (Number of observations: 48) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -2.53      0.14    -2.81    -2.26 1.00     1482     1857\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}